{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# p.176 必要なライブラリをインポートする\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"ggplot\")\n\n# p.178 ランダムシードの設定\nimport random\nnp.random.seed(1234)\nrandom.seed(1234)\n\n# p.184 LabelEncoderのライブラリをインポート\nfrom sklearn.preprocessing import LabelEncoder\n\n# p.188 LightGBMのライブラリをインポート\nimport lightgbm as lgb\n\n# リスト4.17 LightGBMのハイパーパラメータを設定\nlgbm_params = {\n    \"objective\":\"regression\",\n    \"random_seed\":1234\n}\n\n# p.189 クロスバリデーション用のライブラリを読み込んで分割数を3に設定\nfrom sklearn.model_selection import KFold\nfolds = 3\nkf = KFold(n_splits=folds)\n\n# p.190 リスト4.19 平均二乗誤差を出すライブラリをインポート\nfrom sklearn.metrics import mean_squared_error\n\n# p.234 リスト4.65 Optunaのライブラリのインポート\nimport optuna\n\n# p.235 リスト4.66 学習データと検証データを作成\nfrom sklearn.model_selection import train_test_split\n\n# p.245 リスト4.76 ランダムフォレスト用のライブラリの読み込み\nfrom sklearn.ensemble import RandomForestRegressor as rf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-08T13:42:16.535638Z","iopub.execute_input":"2022-01-08T13:42:16.536268Z","iopub.status.idle":"2022-01-08T13:42:19.652383Z","shell.execute_reply.started":"2022-01-08T13:42:16.536153Z","shell.execute_reply":"2022-01-08T13:42:19.651468Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"# p.179 CSVデータを読み込む\ntrain_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n\n# train_df.head()\n\n# p.180 学習データの各変数の型を確認する\n# train_df.dtypes\n\n# p.181 MSZoningの各分類ごとの個数を確認する\n# train_df[\"MSZoning\"].value_counts()\n\n# p.182 学習データとテストデータの連結\nall_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n# all_df\n\n# p.183 目的変数であるSalePriceの値を確認\n# all_df[\"SalePrice\"]\n\n# object型の変数を取得\ncategories = all_df.columns[all_df.dtypes == \"object\"]\n# print(categories)\n\n# p.185 'Alley'の各分類の個数を確認\n# all_df[\"Alley\"].value_counts()\n\n# 欠損値を数値に変換する\nfor cat in categories:\n    le = LabelEncoder()\n    # print(cat)\n    all_df[cat].fillna(\"missing\", inplace=True)\n    le = le.fit(all_df[cat])\n    all_df[cat] = le.transform(all_df[cat])\n    all_df[cat] = all_df[cat].astype(\"category\")\n# all_df\n\n# p.188 データをtrain_dfとtest_dfに戻す\n#train_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\n#test_df_le = all_df[all_df[\"SalePrice\"].isnull()]\n# train_df_le.head()\n# test_df_le.head()\n\n# リスト4.18 説明変数、目的変数を指定\n#train_X = train_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n#train_Y = train_df_le[\"SalePrice\"]\n\n# p.190 リスト4.20 各foldごとに作成したモデルごとの予測値を保存\n#models = []\n#rmses = []\n#oof = np.zeros(len(train_X))\n\n#for train_index, val_index in kf.split(train_X):\n#    X_train = train_X.iloc[train_index]\n#    X_valid = train_X.iloc[val_index]\n#    y_train = train_Y.iloc[train_index]\n#    y_valid = train_Y.iloc[val_index]\n    \n#    lgb_train = lgb.Dataset(X_train, y_train)\n#    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n#    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n#    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n#    tmp_rmse = np.sqrt(mean_squared_error(np.log(y_valid), np.log(y_pred)))\n#    print(tmp_rmse)\n    \n#    models.append(model_lgb)\n#    rmses.append(tmp_rmse)\n#    oof[val_index] = y_pred\n    \n# p.192 平均RMSEを計算する\n# sum(rmses)/len(rmses)\n\n# p.193 statisticsライブラリから計算\n#from statistics import mean\n#mean(rmses)\n\n# p.193 現状の予測値と実際の値の違いを可視化\n#actual_pred_df = pd.DataFrame({\"actual\": train_Y, \"pred\": oof})\n#actual_pred_df.plot(figsize=(12,5))\n\n# p.194 各変数の重要度を確認する\n# リスト4.24 変数の数を制限して各変数の重要度を表示\n#for model in models:\n#    lgb.plot_importance(model, importance_type=\"gain\", max_num_features=15)\n\n# p.197 リスト4.25 SalePriceの各統計量を確認する\n#train_df[\"SalePrice\"].describe()\n\n# p.198 リスト4.26 ヒストグラムで分布を確認\n#train_df[\"SalePrice\"].plot.hist(bins=20)\n\n# p.199 リスト4.27 SalePriceを対数化\n#np.log(train_df['SalePrice'])\n\n# 対数化したSalePriceの分布をヒストグラムで可視化\n#np.log(train_df['SalePrice']).plot.hist(bins=20)\n\n# p.200 リスト4.29 対数化による予測精度の向上を確認\n#train_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\n#train_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\n#train_Y = train_df_le[\"SalePrice_log\"]\n\n#models = []\n#rmses = []\n#oof = np.zeros(len(train_X))\n\n#for train_index, val_index in kf.split(train_X):\n#    X_train = train_X.iloc[train_index]\n#    X_valid = train_X.iloc[val_index]\n#    y_train = train_Y.iloc[train_index]\n#    y_valid = train_Y.iloc[val_index]\n    \n#    lgb_train = lgb.Dataset(X_train, y_train)\n#    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n#    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n#    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n#    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n#    print(tmp_rmse)\n    \n#    models.append(model_lgb)\n#    rmses.append(tmp_rmse)\n#    oof[val_index] = y_pred\n    \n#sum(rmses)/len(rmses)\n\n# p.204 リスト4.30 all_dfの作成\nall_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\ncategories = all_df.columns[all_df.dtypes == \"object\"]\n#print(categories)\n\n# p.205 リスト4.31 欠損値の数が上位40の変数を確認\n#all_df.isnull().sum().sort_values(ascending=False).head(40)\n\n# p.207 リスト4.32 PoolQCの各分類ごとの個数\n#all_df.PoolQC.value_counts()\n\n# リスト4.33 PoolQCの値を値があるものを1、値がないものを0に変換\n#all_df.loc[~all_df[\"PoolQC\"].isnull(), \"PoolQC\"] = 1\n#all_df.loc[all_df[\"PoolQC\"].isnull(), \"PoolQC\"] = 0\n# all_df.PoolQC.value_counts()\n\n# p.208 リスト4.35 MiscFeature, Alleyも0と1に変換する\n#all_df.loc[~all_df[\"MiscFeature\"].isnull(), \"MiscFeature\"] = 1\n#all_df.loc[all_df[\"MiscFeature\"].isnull(), \"MiscFeature\"] = 0\n#all_df.loc[~all_df[\"Alley\"].isnull(), \"Alley\"] = 1\n#all_df.loc[~all_df[\"Alley\"].isnull(), \"Alley\"] = 0\n\n# リスト4.36 繰り返し処理はfor文でまとめる\nHighFacility_col = [\"PoolQC\", \"MiscFeature\", \"Alley\"]\nfor col in HighFacility_col:\n    if all_df[col].dtype == \"object\":\n        if len(all_df[all_df[col].isnull()]) > 0:\n            all_df.loc[~all_df[col].isnull(), col] = 1\n            all_df.loc[all_df[col].isnull(), col] = 0\n\n# p.209 リスト4.37 0か1の値に変換した各変数を足し合わせて、高級住宅設備の数という特徴量を作成\nall_df[\"hasHighFacility\"] = all_df[\"PoolQC\"] + all_df[\"MiscFeature\"] + all_df[\"Alley\"]\nall_df[\"hasHighFacility\"] = all_df[\"hasHighFacility\"].astype(int)\n# all_df.hasHighFacility.value_counts()\n\n# p.209 リスト4.39 もとのデータからPoolQC, MiscFeature, Alleyを削除\nall_df = all_df.drop([\"PoolQC\", \"MiscFeature\", \"Alley\"], axis=1)\n\n# p.211 リスト4.40 各変数の統計量を確認する\n#all_df.describe().T # 転置\n\n# p.212 リスト4.41 数値データのみの抜き出し\ntrain_df_num = train_df.select_dtypes(include=[np.number])\n\n# リスト4.42 比例尺度ではない変数\nnonratio_features = [\"Id\", \"MSSubClass\", \"OverallQual\", \"OverallCond\", \"YearRemodAdd\", \"MoSold\", \"YrSold\"]\n\n# リスト4.43 数値データからリスト4.43の変数を除いた比例尺度データ\nnum_features = sorted(list(set(train_df_num) - set(nonratio_features)))\n#num_features\n\n# p.213 リスト4.44 比例尺度の列のみを抜き出す\ntrain_df_num_rs = train_df_num[num_features]\n\n# p.214 リスト4.45 3/4分位数が0となる変数を確認\n#for col in num_features:\n#    if train_df_num_rs.describe()[col][\"75%\"] == 0:\n#        print(col, len(train_df_num_rs[train_df_num_rs[col] == 0]))\n\n# p.215 リスト4.46 ある特定の値のみしかとらないものを確認\n#for col in num_features:\n#    if train_df_num_rs[col].nunique() < 15:\n#        print(col, train_df_num_rs[col].nunique())\n        \n# リスト4.47 外れ値があるか確認\nfor col in num_features:\n    tmp_df = train_df_num_rs[(train_df_num_rs[col] > train_df_num_rs[col].mean() + train_df_num_rs[col].std()*3) | (train_df_num_rs[col] < train_df_num_rs[col].mean() - train_df_num_rs[col].std()*3)]\n#    print(col, len(tmp_df))\n \n# p.217 リスト4.48 BsmtFinSF1とSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"BsmtFinSF1\", y=\"SalePrice\")\n\n# リスト4.49 BsmtFinSF1が広いもののSalePriceが高くないものを確認\n#all_df[all_df[\"BsmtFinSF1\"] > 5000]\n\n# p.218 TotalBsmtSFとSalePriceの分布を可視化\n# all_df.plot.scatter(x=\"TotalBsmtSF\", y=\"SalePrice\")\n# all_df[all_df[\"TotalBsmtSF\"] > 6000]\n\n# p.219 GrLivAreaとSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"GrLivArea\", y=\"SalePrice\")\n#all_df[all_df[\"GrLivArea\"] > 5000]\n\n# p.219 1stFlrSFとSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"1stFlrSF\", y=\"SalePrice\")\n#all_df[all_df[\"1stFlrSF\"] > 4000]\n\n# p.221 リスト4.53 外れ値以外を抽出（テストデータはすべて抽出）\nall_df = all_df[(all_df['BsmtFinSF1'] < 2000) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['TotalBsmtSF'] < 3000) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['GrLivArea'] < 4500) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['1stFlrSF'] < 2500) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['LotArea'] < 100000) | (all_df['SalePrice'].isnull())]\n\n# リスト4.54 categoriesの中から除外した3つの変数を削除\ncategories = categories.drop([\"PoolQC\", \"MiscFeature\", \"Alley\"])\n\n# リスト4.55 欠損値をmissingに置き換えてall_dfのカテゴリ変数をcategoryに設定\nfor cat in categories:\n    le = LabelEncoder()\n    #print(cat)\n    \n    all_df[cat].fillna(\"missing\", inplace=True)\n    le = le.fit(all_df[cat])\n    all_df[cat] = le.transform(all_df[cat])\n    all_df[cat] = all_df[cat].astype(\"category\")\n\n\n# p.228 リスト4.58 特徴量を追加\nall_df[\"Age\"] = all_df[\"YrSold\"] - all_df[\"YearBuilt\"]\n       \n# p.230 リスト4.61 広さの変数から追加するもの\nall_df[\"TotalSF\"] = all_df[\"TotalBsmtSF\"] + all_df[\"1stFlrSF\"] + all_df[\"2ndFlrSF\"]\nall_df[\"Total_Bathrooms\"] = all_df[\"FullBath\"] + all_df[\"HalfBath\"] + all_df[\"BsmtFullBath\"] + all_df[\"BsmtHalfBath\"]\n\n# リスト4.62 Porchの広さの合計も特徴量として追加\nall_df[\"Total_PorchSF\"] = all_df[\"WoodDeckSF\"] + all_df[\"OpenPorchSF\"] + all_df[\"EnclosedPorch\"] + all_df[\"3SsnPorch\"] + all_df[\"ScreenPorch\"]\n\n# リスト4.63 Porchの広さの合計をPorchがあるかないかの0,1の値に変換\nall_df[\"hasPorch\"] = all_df[\"Total_PorchSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_df = all_df.drop(\"Total_PorchSF\", axis=1)   \n\n\n# p.223 リスト4.56 train_df_leとtest_df_leに分割\ntrain_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\ntest_df_le = all_df[all_df[\"SalePrice\"].isnull()]\n\ntrain_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\ntrain_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\ntrain_Y = train_df_le[\"SalePrice_log\"]\n\n# p.239 得られたハイパーパラメータを設定してクロスバリデーション\n# 適用前 valid_0's l2: 0.0137713\n# 適用後 valid_0's l2: 0.0129672\nlgbm_params = {\n    'objective': 'regression',\n    'random_seed':1234,\n    'learning_rate':0.05,\n    'n_estimators':1000,\n    'num_leaves': 33,\n    'max_bin': 125,\n    'bagging_fraction': 0.7197362581993618,\n    'bagging_freq': 4,\n     'feature_fraction': 0.4684501358427995,\n     'min_data_in_leaf': 14,\n     'min_sum_hessian_in_leaf': 2\n}\n\n\nmodels = []\nrmses = []\noof = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    #print(tmp_rmse)\n    \n    models.append(model_lgb)\n    rmses.append(tmp_rmse)\n    oof[val_index] = y_pred\n\n# sum(rmses)/len(rmses)\n    \n# p.227 リスト4.57 時間に関する変数の統計量を確認\n#all_df[[\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\",\"YrSold\"]].describe()\n\n# p.229 リスト4.60 広さに関する変数の統計量を確認\n#all_df[[\"LotArea\", \"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"LotFrontage\"]].describe()\n\n# p.241 Kaggleに結果をsubmitする\n# リスト4.69 テストデータを用意\ntest_X = test_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n\n# リスト4.70 クロスバリデーションごとの各モデルで予測値を算出\npreds = []\n\nfor model in models:\n    pred = model.predict(test_X)\n    preds.append(pred)\n    \n# p.242 リスト4.71 predsの平均を計算してpreds_meanとして取得\npreds_array = np.array(preds)\npreds_mean = np.mean(preds_array, axis=0)\n\n# リスト4.72 もとのスケールに戻す\npreds_exp = np.exp(preds_mean)\nlen(preds_exp)\n\n# リスト4.73 予測値をSalePriceの値として置き換え\nsubmission[\"SalePrice\"] = preds_exp\n\n# p.243 リスト4.75 CSVファイルとして書き出す\n# submission.to_csv(\"houseprices_submit01.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:42:19.654563Z","iopub.execute_input":"2022-01-08T13:42:19.654785Z","iopub.status.idle":"2022-01-08T13:42:21.571650Z","shell.execute_reply.started":"2022-01-08T13:42:19.654760Z","shell.execute_reply":"2022-01-08T13:42:21.570962Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:256: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003876 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2374\n[LightGBM] [Info] Number of data points in the train set: 963, number of used features: 77\n[LightGBM] [Info] Start training from score 12.010615\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's l2: 0.0730654\n[20]\tvalid_0's l2: 0.039388\n[30]\tvalid_0's l2: 0.0260691\n[40]\tvalid_0's l2: 0.019713\n[50]\tvalid_0's l2: 0.0167307\n[60]\tvalid_0's l2: 0.0153157\n[70]\tvalid_0's l2: 0.0145976\n[80]\tvalid_0's l2: 0.0141165\n[90]\tvalid_0's l2: 0.0139484\n[100]\tvalid_0's l2: 0.0137097\n[110]\tvalid_0's l2: 0.013655\n[120]\tvalid_0's l2: 0.0135674\n[130]\tvalid_0's l2: 0.0135293\n[140]\tvalid_0's l2: 0.0135037\n[150]\tvalid_0's l2: 0.0134944\n[160]\tvalid_0's l2: 0.013517\nEarly stopping, best iteration is:\n[143]\tvalid_0's l2: 0.0134783\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000647 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2370\n[LightGBM] [Info] Number of data points in the train set: 963, number of used features: 78\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Start training from score 12.018820\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's l2: 0.0833306\n[20]\tvalid_0's l2: 0.0493842\n[30]\tvalid_0's l2: 0.0337997\n[40]\tvalid_0's l2: 0.0266571\n[50]\tvalid_0's l2: 0.0230607\n[60]\tvalid_0's l2: 0.0214028\n[70]\tvalid_0's l2: 0.0202047\n[80]\tvalid_0's l2: 0.0195021\n[90]\tvalid_0's l2: 0.0189893\n[100]\tvalid_0's l2: 0.0187437\n[110]\tvalid_0's l2: 0.0185144\n[120]\tvalid_0's l2: 0.0184643\n[130]\tvalid_0's l2: 0.0183792\n[140]\tvalid_0's l2: 0.0183163\n[150]\tvalid_0's l2: 0.01836\n[160]\tvalid_0's l2: 0.0182416\n[170]\tvalid_0's l2: 0.0181873\n[180]\tvalid_0's l2: 0.0181668\n[190]\tvalid_0's l2: 0.0182043\n[200]\tvalid_0's l2: 0.0181913\nEarly stopping, best iteration is:\n[185]\tvalid_0's l2: 0.0181394\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2389\n[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 78\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Start training from score 12.021869\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's l2: 0.0672398\n[20]\tvalid_0's l2: 0.0368451\n[30]\tvalid_0's l2: 0.0242847\n[40]\tvalid_0's l2: 0.0184089\n[50]\tvalid_0's l2: 0.0157186\n[60]\tvalid_0's l2: 0.0144352\n[70]\tvalid_0's l2: 0.0138335\n[80]\tvalid_0's l2: 0.0135654\n[90]\tvalid_0's l2: 0.0134322\n[100]\tvalid_0's l2: 0.0133707\n[110]\tvalid_0's l2: 0.0132919\n[120]\tvalid_0's l2: 0.0131674\n[130]\tvalid_0's l2: 0.0130868\n[140]\tvalid_0's l2: 0.0130366\n[150]\tvalid_0's l2: 0.0129945\n[160]\tvalid_0's l2: 0.0130438\nEarly stopping, best iteration is:\n[144]\tvalid_0's l2: 0.0129672\n","output_type":"stream"}]},{"cell_type":"code","source":"# p.234 Optunaを実装する\n\n# p.235 リスト4.66 学習データと検証データを作成\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234, shuffle=False, stratify=None)\n\n# p.235 リスト4.67 Optunaでハイパーパラメータを最適化する\ndef objective(trial):\n    params = {\n        \"Objective\":\"regression\",\n        \"random_seed\":1234,\n        \"learning_rate\":0.05,\n        \"n_estimators\":1000,\n        \n        \"num_leaves\":trial.suggest_int(\"num_leaves\", 4, 64),\n        \"max_bin\":trial.suggest_int(\"max_bin\", 50, 200),\n        \"bagging_fraction\":trial.suggest_uniform(\"bagging_fraction\", 0.4, 0.9),\n        \"bagging_freq\":trial.suggest_int(\"bagging_freq\", 1, 10),\n        \"feature_fraction\":trial.suggest_uniform(\"feature_fraction\", 0.4, 0.9),\n        \"min_data_in_leaf\":trial.suggest_int(\"min_data_in_leaf\", 2, 16),\n        \"min_sum_hessian_in_leaf\":trial.suggest_int(\"min_sum_hessian_in_leaf\", 1, 10),\n    }\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n    model_lgb = lgb.train(params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n    score = np.sqrt(mean_squared_error(y_valid, y_pred))\n    \n    return score\n\n#study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n#study.optimize(objective, n_trials=50)\n#study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-01-08T13:42:21.575732Z","iopub.execute_input":"2022-01-08T13:42:21.578036Z","iopub.status.idle":"2022-01-08T13:42:21.595521Z","shell.execute_reply.started":"2022-01-08T13:42:21.577987Z","shell.execute_reply":"2022-01-08T13:42:21.594833Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# p.245 欠損値を含む変数を確認する\nhasnan_cat = []\n\nfor col in all_df.columns:\n    tmp_null_count = all_df[col].isnull().sum()\n    if (tmp_null_count > 0) & (col != \"SalePrice\"):\n        #print(col, tmp_null_count)\n        hasnan_cat.append(col)\n        \n# p.246 欠損値を含む変数の統計量を確認する\n# リスト4.78 hasnan_catに含まれる変数を確認\n# all_df[hasnan_cat].describe()\n\n# p.247 欠損値を各変数の中央値で補完する\nfor col in all_df.columns:\n    tmp_null_count = all_df[col].isnull().sum()\n    if (tmp_null_count > 0) & (col != \"SalePrice\"):\n        print(col, tmp_null_count)\n        all_df[col] = all_df[col].fillna(all_df[col].median())\n\n# p.248 リスト4.80 SalePriceの対数をとって学習\ntrain_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\ntest_df_le = all_df[all_df[\"SalePrice\"].isnull()]\ntrain_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\n\ntrain_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\ntrain_Y = train_df_le[\"SalePrice_log\"]\n\nfolds = 3\nkdf = KFold(n_splits=folds)\nmodels_rf = []\nrmses_rf = []\noof_rf = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    model_rf = rf(\n        n_estimators=50,\n        random_state=1234\n    )\n    \n    model_rf.fit(X_train, y_train)\n    \n    y_pred = model_rf.predict(X_valid)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    #print(tmp_rmse)\n    \n    models_rf.append(model_rf)\n    rmses_rf.append(tmp_rmse)\n    oof_rf[val_index] = y_pred\n    \n#sum(rmses_rf)/len(rmses_rf)\n\n# p.249 リスト4.81 テストデータで各クロスバリデーションのモデルで予測値を算出\ntest_X = test_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n\npreds_rf = []\nfor model in models_rf:\n    pred = model.predict(test_X)\n    preds_rf.append(pred)\n    \npreds_array_rf = np.array(preds_rf)\npreds_mean_rf = np.mean(preds_array_rf, axis=0)\npreds_exp_rf = np.exp(preds_mean_rf)\nsubmission[\"SalePrice\"] = preds_exp_rf\n\n#submission.to_csv(\"houseprices_submit02.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-08T14:02:06.495440Z","iopub.execute_input":"2022-01-08T14:02:06.495728Z","iopub.status.idle":"2022-01-08T14:02:09.224564Z","shell.execute_reply.started":"2022-01-08T14:02:06.495700Z","shell.execute_reply":"2022-01-08T14:02:09.223627Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","output_type":"stream"}]}]}