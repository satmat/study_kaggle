{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# p.176 必要なライブラリをインポートする\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"ggplot\")\n\n# p.178 ランダムシードの設定\nimport random\nnp.random.seed(1234)\nrandom.seed(1234)\n\n# p.184 LabelEncoderのライブラリをインポート\nfrom sklearn.preprocessing import LabelEncoder\n\n# p.188 LightGBMのライブラリをインポート\nimport lightgbm as lgb\n\n# リスト4.17 LightGBMのハイパーパラメータを設定\nlgbm_params = {\n    \"objective\":\"regression\",\n    \"random_seed\":1234\n}\n\n# p.189 クロスバリデーション用のライブラリを読み込んで分割数を3に設定\nfrom sklearn.model_selection import KFold\nfolds = 3\nkf = KFold(n_splits=folds)\n\n# p.190 リスト4.19 平均二乗誤差を出すライブラリをインポート\nfrom sklearn.metrics import mean_squared_error\n\n# p.234 リスト4.65 Optunaのライブラリのインポート\nimport optuna\n\n# p.235 リスト4.66 学習データと検証データを作成\nfrom sklearn.model_selection import train_test_split\n\n# p.245 リスト4.76 ランダムフォレスト用のライブラリの読み込み\nfrom sklearn.ensemble import RandomForestRegressor as rf\n\n# p.251 リスト4.84 XGBoostのライブラリのインポート\nimport xgboost as xgb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-10T13:22:22.778359Z","iopub.execute_input":"2022-01-10T13:22:22.779165Z","iopub.status.idle":"2022-01-10T13:22:25.707460Z","shell.execute_reply.started":"2022-01-10T13:22:22.779052Z","shell.execute_reply":"2022-01-10T13:22:25.706609Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"# p.179 CSVデータを読み込む\ntrain_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n\n# train_df.head()\n\n# p.180 学習データの各変数の型を確認する\n# train_df.dtypes\n\n# p.181 MSZoningの各分類ごとの個数を確認する\n# train_df[\"MSZoning\"].value_counts()\n\n# p.182 学習データとテストデータの連結\nall_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n# all_df\n\n# p.183 目的変数であるSalePriceの値を確認\n# all_df[\"SalePrice\"]\n\n# object型の変数を取得\ncategories = all_df.columns[all_df.dtypes == \"object\"]\n# print(categories)\n\n# p.185 'Alley'の各分類の個数を確認\n# all_df[\"Alley\"].value_counts()\n\n# 欠損値を数値に変換する\nfor cat in categories:\n    le = LabelEncoder()\n    # print(cat)\n    all_df[cat].fillna(\"missing\", inplace=True)\n    le = le.fit(all_df[cat])\n    all_df[cat] = le.transform(all_df[cat])\n    all_df[cat] = all_df[cat].astype(\"category\")\n# all_df\n\n# p.188 データをtrain_dfとtest_dfに戻す\n#train_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\n#test_df_le = all_df[all_df[\"SalePrice\"].isnull()]\n# train_df_le.head()\n# test_df_le.head()\n\n# リスト4.18 説明変数、目的変数を指定\n#train_X = train_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n#train_Y = train_df_le[\"SalePrice\"]\n\n# p.190 リスト4.20 各foldごとに作成したモデルごとの予測値を保存\n#models = []\n#rmses = []\n#oof = np.zeros(len(train_X))\n\n#for train_index, val_index in kf.split(train_X):\n#    X_train = train_X.iloc[train_index]\n#    X_valid = train_X.iloc[val_index]\n#    y_train = train_Y.iloc[train_index]\n#    y_valid = train_Y.iloc[val_index]\n    \n#    lgb_train = lgb.Dataset(X_train, y_train)\n#    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n#    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n#    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n#    tmp_rmse = np.sqrt(mean_squared_error(np.log(y_valid), np.log(y_pred)))\n#    print(tmp_rmse)\n    \n#    models.append(model_lgb)\n#    rmses.append(tmp_rmse)\n#    oof[val_index] = y_pred\n    \n# p.192 平均RMSEを計算する\n# sum(rmses)/len(rmses)\n\n# p.193 statisticsライブラリから計算\n#from statistics import mean\n#mean(rmses)\n\n# p.193 現状の予測値と実際の値の違いを可視化\n#actual_pred_df = pd.DataFrame({\"actual\": train_Y, \"pred\": oof})\n#actual_pred_df.plot(figsize=(12,5))\n\n# p.194 各変数の重要度を確認する\n# リスト4.24 変数の数を制限して各変数の重要度を表示\n#for model in models:\n#    lgb.plot_importance(model, importance_type=\"gain\", max_num_features=15)\n\n# p.197 リスト4.25 SalePriceの各統計量を確認する\n#train_df[\"SalePrice\"].describe()\n\n# p.198 リスト4.26 ヒストグラムで分布を確認\n#train_df[\"SalePrice\"].plot.hist(bins=20)\n\n# p.199 リスト4.27 SalePriceを対数化\n#np.log(train_df['SalePrice'])\n\n# 対数化したSalePriceの分布をヒストグラムで可視化\n#np.log(train_df['SalePrice']).plot.hist(bins=20)\n\n# p.200 リスト4.29 対数化による予測精度の向上を確認\n#train_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\n#train_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\n#train_Y = train_df_le[\"SalePrice_log\"]\n\n#models = []\n#rmses = []\n#oof = np.zeros(len(train_X))\n\n#for train_index, val_index in kf.split(train_X):\n#    X_train = train_X.iloc[train_index]\n#    X_valid = train_X.iloc[val_index]\n#    y_train = train_Y.iloc[train_index]\n#    y_valid = train_Y.iloc[val_index]\n    \n#    lgb_train = lgb.Dataset(X_train, y_train)\n#    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n#    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n#    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n#    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n#    print(tmp_rmse)\n    \n#    models.append(model_lgb)\n#    rmses.append(tmp_rmse)\n#    oof[val_index] = y_pred\n    \n#sum(rmses)/len(rmses)\n\n# p.204 リスト4.30 all_dfの作成\nall_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\ncategories = all_df.columns[all_df.dtypes == \"object\"]\n#print(categories)\n\n# p.205 リスト4.31 欠損値の数が上位40の変数を確認\n#all_df.isnull().sum().sort_values(ascending=False).head(40)\n\n# p.207 リスト4.32 PoolQCの各分類ごとの個数\n#all_df.PoolQC.value_counts()\n\n# リスト4.33 PoolQCの値を値があるものを1、値がないものを0に変換\n#all_df.loc[~all_df[\"PoolQC\"].isnull(), \"PoolQC\"] = 1\n#all_df.loc[all_df[\"PoolQC\"].isnull(), \"PoolQC\"] = 0\n# all_df.PoolQC.value_counts()\n\n# p.208 リスト4.35 MiscFeature, Alleyも0と1に変換する\n#all_df.loc[~all_df[\"MiscFeature\"].isnull(), \"MiscFeature\"] = 1\n#all_df.loc[all_df[\"MiscFeature\"].isnull(), \"MiscFeature\"] = 0\n#all_df.loc[~all_df[\"Alley\"].isnull(), \"Alley\"] = 1\n#all_df.loc[~all_df[\"Alley\"].isnull(), \"Alley\"] = 0\n\n# リスト4.36 繰り返し処理はfor文でまとめる\nHighFacility_col = [\"PoolQC\", \"MiscFeature\", \"Alley\"]\nfor col in HighFacility_col:\n    if all_df[col].dtype == \"object\":\n        if len(all_df[all_df[col].isnull()]) > 0:\n            all_df.loc[~all_df[col].isnull(), col] = 1\n            all_df.loc[all_df[col].isnull(), col] = 0\n\n# p.209 リスト4.37 0か1の値に変換した各変数を足し合わせて、高級住宅設備の数という特徴量を作成\nall_df[\"hasHighFacility\"] = all_df[\"PoolQC\"] + all_df[\"MiscFeature\"] + all_df[\"Alley\"]\nall_df[\"hasHighFacility\"] = all_df[\"hasHighFacility\"].astype(int)\n# all_df.hasHighFacility.value_counts()\n\n# p.209 リスト4.39 もとのデータからPoolQC, MiscFeature, Alleyを削除\nall_df = all_df.drop([\"PoolQC\", \"MiscFeature\", \"Alley\"], axis=1)\n\n# p.211 リスト4.40 各変数の統計量を確認する\n#all_df.describe().T # 転置\n\n# p.212 リスト4.41 数値データのみの抜き出し\ntrain_df_num = train_df.select_dtypes(include=[np.number])\n\n# リスト4.42 比例尺度ではない変数\nnonratio_features = [\"Id\", \"MSSubClass\", \"OverallQual\", \"OverallCond\", \"YearRemodAdd\", \"MoSold\", \"YrSold\"]\n\n# リスト4.43 数値データからリスト4.43の変数を除いた比例尺度データ\nnum_features = sorted(list(set(train_df_num) - set(nonratio_features)))\n#num_features\n\n# p.213 リスト4.44 比例尺度の列のみを抜き出す\ntrain_df_num_rs = train_df_num[num_features]\n\n# p.214 リスト4.45 3/4分位数が0となる変数を確認\n#for col in num_features:\n#    if train_df_num_rs.describe()[col][\"75%\"] == 0:\n#        print(col, len(train_df_num_rs[train_df_num_rs[col] == 0]))\n\n# p.215 リスト4.46 ある特定の値のみしかとらないものを確認\n#for col in num_features:\n#    if train_df_num_rs[col].nunique() < 15:\n#        print(col, train_df_num_rs[col].nunique())\n        \n# リスト4.47 外れ値があるか確認\nfor col in num_features:\n    tmp_df = train_df_num_rs[(train_df_num_rs[col] > train_df_num_rs[col].mean() + train_df_num_rs[col].std()*3) | (train_df_num_rs[col] < train_df_num_rs[col].mean() - train_df_num_rs[col].std()*3)]\n#    print(col, len(tmp_df))\n \n# p.217 リスト4.48 BsmtFinSF1とSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"BsmtFinSF1\", y=\"SalePrice\")\n\n# リスト4.49 BsmtFinSF1が広いもののSalePriceが高くないものを確認\n#all_df[all_df[\"BsmtFinSF1\"] > 5000]\n\n# p.218 TotalBsmtSFとSalePriceの分布を可視化\n# all_df.plot.scatter(x=\"TotalBsmtSF\", y=\"SalePrice\")\n# all_df[all_df[\"TotalBsmtSF\"] > 6000]\n\n# p.219 GrLivAreaとSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"GrLivArea\", y=\"SalePrice\")\n#all_df[all_df[\"GrLivArea\"] > 5000]\n\n# p.219 1stFlrSFとSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"1stFlrSF\", y=\"SalePrice\")\n#all_df[all_df[\"1stFlrSF\"] > 4000]\n\n# p.221 リスト4.53 外れ値以外を抽出（テストデータはすべて抽出）\nall_df = all_df[(all_df['BsmtFinSF1'] < 2000) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['TotalBsmtSF'] < 3000) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['GrLivArea'] < 4500) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['1stFlrSF'] < 2500) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['LotArea'] < 100000) | (all_df['SalePrice'].isnull())]\n\n# リスト4.54 categoriesの中から除外した3つの変数を削除\ncategories = categories.drop([\"PoolQC\", \"MiscFeature\", \"Alley\"])\n\n# リスト4.55 欠損値をmissingに置き換えてall_dfのカテゴリ変数をcategoryに設定\nfor cat in categories:\n    le = LabelEncoder()\n    #print(cat)\n    \n    all_df[cat].fillna(\"missing\", inplace=True)\n    le = le.fit(all_df[cat])\n    all_df[cat] = le.transform(all_df[cat])\n    all_df[cat] = all_df[cat].astype(\"category\")\n\n\n# p.228 リスト4.58 特徴量を追加\nall_df[\"Age\"] = all_df[\"YrSold\"] - all_df[\"YearBuilt\"]\n       \n# p.230 リスト4.61 広さの変数から追加するもの\nall_df[\"TotalSF\"] = all_df[\"TotalBsmtSF\"] + all_df[\"1stFlrSF\"] + all_df[\"2ndFlrSF\"]\nall_df[\"Total_Bathrooms\"] = all_df[\"FullBath\"] + all_df[\"HalfBath\"] + all_df[\"BsmtFullBath\"] + all_df[\"BsmtHalfBath\"]\n\n# リスト4.62 Porchの広さの合計も特徴量として追加\nall_df[\"Total_PorchSF\"] = all_df[\"WoodDeckSF\"] + all_df[\"OpenPorchSF\"] + all_df[\"EnclosedPorch\"] + all_df[\"3SsnPorch\"] + all_df[\"ScreenPorch\"]\n\n# リスト4.63 Porchの広さの合計をPorchがあるかないかの0,1の値に変換\nall_df[\"hasPorch\"] = all_df[\"Total_PorchSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_df = all_df.drop(\"Total_PorchSF\", axis=1)   \n\n\n# p.223 リスト4.56 train_df_leとtest_df_leに分割\ntrain_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\ntest_df_le = all_df[all_df[\"SalePrice\"].isnull()]\n\ntrain_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\ntrain_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\ntrain_Y = train_df_le[\"SalePrice_log\"]\n\n# p.239 得られたハイパーパラメータを設定してクロスバリデーション\n# 適用前 valid_0's l2: 0.0137713\n# 適用後 valid_0's l2: 0.0129672\nlgbm_params = {\n    'objective': 'regression',\n    'random_seed':1234,\n    'learning_rate':0.05,\n    'n_estimators':1000,\n    'num_leaves': 33,\n    'max_bin': 125,\n    'bagging_fraction': 0.7197362581993618,\n    'bagging_freq': 4,\n     'feature_fraction': 0.4684501358427995,\n     'min_data_in_leaf': 14,\n     'min_sum_hessian_in_leaf': 2\n}\n\n\nmodels = []\nrmses = []\noof = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    #print(tmp_rmse)\n    \n    models.append(model_lgb)\n    rmses.append(tmp_rmse)\n    oof[val_index] = y_pred\n\n# sum(rmses)/len(rmses)\n    \n# p.227 リスト4.57 時間に関する変数の統計量を確認\n#all_df[[\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\",\"YrSold\"]].describe()\n\n# p.229 リスト4.60 広さに関する変数の統計量を確認\n#all_df[[\"LotArea\", \"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"LotFrontage\"]].describe()\n\n# p.241 Kaggleに結果をsubmitする\n# リスト4.69 テストデータを用意\ntest_X = test_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n\n# リスト4.70 クロスバリデーションごとの各モデルで予測値を算出\npreds = []\n\nfor model in models:\n    pred = model.predict(test_X)\n    preds.append(pred)\n    \n# p.242 リスト4.71 predsの平均を計算してpreds_meanとして取得\npreds_array = np.array(preds)\npreds_mean = np.mean(preds_array, axis=0)\n\n# リスト4.72 もとのスケールに戻す\npreds_exp = np.exp(preds_mean)\nlen(preds_exp)\n\n# リスト4.73 予測値をSalePriceの値として置き換え\nsubmission[\"SalePrice\"] = preds_exp\n\n# p.243 リスト4.75 CSVファイルとして書き出す\n# submission.to_csv(\"houseprices_submit01.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:22:25.709541Z","iopub.execute_input":"2022-01-10T13:22:25.709832Z","iopub.status.idle":"2022-01-10T13:22:27.470649Z","shell.execute_reply.started":"2022-01-10T13:22:25.709797Z","shell.execute_reply":"2022-01-10T13:22:27.469677Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:256: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002670 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2374\n[LightGBM] [Info] Number of data points in the train set: 963, number of used features: 77\n[LightGBM] [Info] Start training from score 12.010615\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's l2: 0.0730654\n[20]\tvalid_0's l2: 0.039388\n[30]\tvalid_0's l2: 0.0260691\n[40]\tvalid_0's l2: 0.019713\n[50]\tvalid_0's l2: 0.0167307\n[60]\tvalid_0's l2: 0.0153157\n[70]\tvalid_0's l2: 0.0145976\n[80]\tvalid_0's l2: 0.0141165\n[90]\tvalid_0's l2: 0.0139484\n[100]\tvalid_0's l2: 0.0137097\n[110]\tvalid_0's l2: 0.013655\n[120]\tvalid_0's l2: 0.0135674\n[130]\tvalid_0's l2: 0.0135293\n[140]\tvalid_0's l2: 0.0135037\n[150]\tvalid_0's l2: 0.0134944\n[160]\tvalid_0's l2: 0.013517\nEarly stopping, best iteration is:\n[143]\tvalid_0's l2: 0.0134783\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000597 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2370\n[LightGBM] [Info] Number of data points in the train set: 963, number of used features: 78\n[LightGBM] [Info] Start training from score 12.018820\nTraining until validation scores don't improve for 20 rounds\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[10]\tvalid_0's l2: 0.0833306\n[20]\tvalid_0's l2: 0.0493842\n[30]\tvalid_0's l2: 0.0337997\n[40]\tvalid_0's l2: 0.0266571\n[50]\tvalid_0's l2: 0.0230607\n[60]\tvalid_0's l2: 0.0214028\n[70]\tvalid_0's l2: 0.0202047\n[80]\tvalid_0's l2: 0.0195021\n[90]\tvalid_0's l2: 0.0189893\n[100]\tvalid_0's l2: 0.0187437\n[110]\tvalid_0's l2: 0.0185144\n[120]\tvalid_0's l2: 0.0184643\n[130]\tvalid_0's l2: 0.0183792\n[140]\tvalid_0's l2: 0.0183163\n[150]\tvalid_0's l2: 0.01836\n[160]\tvalid_0's l2: 0.0182416\n[170]\tvalid_0's l2: 0.0181873\n[180]\tvalid_0's l2: 0.0181668\n[190]\tvalid_0's l2: 0.0182043\n[200]\tvalid_0's l2: 0.0181913\nEarly stopping, best iteration is:\n[185]\tvalid_0's l2: 0.0181394\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2389\n[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 78\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:177: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Start training from score 12.021869\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's l2: 0.0672398\n[20]\tvalid_0's l2: 0.0368451\n[30]\tvalid_0's l2: 0.0242847\n[40]\tvalid_0's l2: 0.0184089\n[50]\tvalid_0's l2: 0.0157186\n[60]\tvalid_0's l2: 0.0144352\n[70]\tvalid_0's l2: 0.0138335\n[80]\tvalid_0's l2: 0.0135654\n[90]\tvalid_0's l2: 0.0134322\n[100]\tvalid_0's l2: 0.0133707\n[110]\tvalid_0's l2: 0.0132919\n[120]\tvalid_0's l2: 0.0131674\n[130]\tvalid_0's l2: 0.0130868\n[140]\tvalid_0's l2: 0.0130366\n[150]\tvalid_0's l2: 0.0129945\n[160]\tvalid_0's l2: 0.0130438\nEarly stopping, best iteration is:\n[144]\tvalid_0's l2: 0.0129672\n","output_type":"stream"}]},{"cell_type":"code","source":"# p.234 Optunaを実装する\n\n# p.235 リスト4.66 学習データと検証データを作成\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234, shuffle=False, stratify=None)\n\n# p.235 リスト4.67 Optunaでハイパーパラメータを最適化する\ndef objective(trial):\n    params = {\n        \"Objective\":\"regression\",\n        \"random_seed\":1234,\n        \"learning_rate\":0.05,\n        \"n_estimators\":1000,\n        \n        \"num_leaves\":trial.suggest_int(\"num_leaves\", 4, 64),\n        \"max_bin\":trial.suggest_int(\"max_bin\", 50, 200),\n        \"bagging_fraction\":trial.suggest_uniform(\"bagging_fraction\", 0.4, 0.9),\n        \"bagging_freq\":trial.suggest_int(\"bagging_freq\", 1, 10),\n        \"feature_fraction\":trial.suggest_uniform(\"feature_fraction\", 0.4, 0.9),\n        \"min_data_in_leaf\":trial.suggest_int(\"min_data_in_leaf\", 2, 16),\n        \"min_sum_hessian_in_leaf\":trial.suggest_int(\"min_sum_hessian_in_leaf\", 1, 10),\n    }\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n    model_lgb = lgb.train(params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n    score = np.sqrt(mean_squared_error(y_valid, y_pred))\n    \n    return score\n\n#study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n#study.optimize(objective, n_trials=50)\n#study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:22:27.474978Z","iopub.execute_input":"2022-01-10T13:22:27.475268Z","iopub.status.idle":"2022-01-10T13:22:27.487832Z","shell.execute_reply.started":"2022-01-10T13:22:27.475237Z","shell.execute_reply":"2022-01-10T13:22:27.486637Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# p.245 欠損値を含む変数を確認する\nhasnan_cat = []\n\nfor col in all_df.columns:\n    tmp_null_count = all_df[col].isnull().sum()\n    if (tmp_null_count > 0) & (col != \"SalePrice\"):\n        #print(col, tmp_null_count)\n        hasnan_cat.append(col)\n        \n# p.246 欠損値を含む変数の統計量を確認する\n# リスト4.78 hasnan_catに含まれる変数を確認\n# all_df[hasnan_cat].describe()\n\n# p.247 欠損値を各変数の中央値で補完する\nfor col in all_df.columns:\n    tmp_null_count = all_df[col].isnull().sum()\n    if (tmp_null_count > 0) & (col != \"SalePrice\"):\n        #print(col, tmp_null_count)\n        all_df[col] = all_df[col].fillna(all_df[col].median())\n\n# p.248 リスト4.80 SalePriceの対数をとって学習\ntrain_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\ntest_df_le = all_df[all_df[\"SalePrice\"].isnull()]\ntrain_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\n\ntrain_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\ntrain_Y = train_df_le[\"SalePrice_log\"]\n\nfolds = 3\nkdf = KFold(n_splits=folds)\nmodels_rf = []\nrmses_rf = []\noof_rf = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    model_rf = rf(\n        n_estimators=50,\n        random_state=1234\n    )\n    \n    model_rf.fit(X_train, y_train)\n    \n    y_pred = model_rf.predict(X_valid)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    #print(tmp_rmse)\n    \n    models_rf.append(model_rf)\n    rmses_rf.append(tmp_rmse)\n    oof_rf[val_index] = y_pred\n    \n#sum(rmses_rf)/len(rmses_rf)\n\n# p.249 リスト4.81 テストデータで各クロスバリデーションのモデルで予測値を算出\ntest_X = test_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n\npreds_rf = []\nfor model in models_rf:\n    pred = model.predict(test_X)\n    preds_rf.append(pred)\n    \npreds_array_rf = np.array(preds_rf)\npreds_mean_rf = np.mean(preds_array_rf, axis=0)\npreds_exp_rf = np.exp(preds_mean_rf)\nsubmission[\"SalePrice\"] = preds_exp_rf\n\n#submission.to_csv(\"houseprices_submit02.csv\", index=False)\n\n\n# p.251 XGBoostを実装する\n# リスト4.85 category変数をint型に変換する\ncategories = train_X.columns[train_X.dtypes == \"category\"]\n\nfor col in categories:\n    train_X[col] = train_X[col].astype(\"int8\")\n    test_X[col] = test_X[col].astype(\"int8\")\n    \n# p.255 リスト4.87 ハイパーパラメータの設定\nxgb_params = {\n    \"learning_rate\":0.05,\n    \"seed\":1234,\n    \"max_depth\": 5,\n    \"colsample_bytree\": 0.3129166625194974,\n    \"sublsample\": 0.6571758278257789\n}\n\n# p.256 XGBoostでモデルを学習する\n# リスト4.88 最適化の処理\nmodels_xgb = []\nrmses_xgb = []\noof_xgb = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    \n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_eval = xgb.DMatrix(X_valid, label=y_valid)\n    evals = [(xgb_train, \"train\"), (xgb_eval, \"eval\")]\n    \n    model_xgb = xgb.train(xgb_params, xgb_train, evals=evals, num_boost_round=1000, early_stopping_rounds=20, verbose_eval=20,)\n    \n    y_pred = model_xgb.predict(xgb_eval)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    print(tmp_rmse)\n    \n    models_xgb.append(model_xgb)\n    rmses_xgb.append(tmp_rmse)\n    oof_xgb[val_index] = y_pred\n\nsum(rmses_xgb)/len(rmses_xgb)\n# 0.11849101592335805\n\n# 結果をCSVファイルとして書き出す\n# p.258 リスト4.89 テストデータでの予測値を算出\nxgb_test = xgb.DMatrix(test_X)\npreds_xgb = []\nfor model in models_xgb:\n    pred = model.predict(xgb_test)\n    preds_xgb.append(pred)\npreds_array_xgb = np.array(preds_xgb)\npreds_mean_xgb = np.mean(preds_array_xgb, axis=0)\npreds_exp_xgb = np.exp(preds_mean_xgb)\nsubmission[\"SalePrice\"] = preds_exp_xgb\n\n# p.259 リスト4.91\n# submission.to_csv(\"houseprices_submit03.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:34:16.419292Z","iopub.execute_input":"2022-01-10T13:34:16.420809Z","iopub.status.idle":"2022-01-10T13:34:28.480580Z","shell.execute_reply.started":"2022-01-10T13:34:16.420749Z","shell.execute_reply":"2022-01-10T13:34:28.479934Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","output_type":"stream"},{"name":"stdout","text":"[13:34:19] WARNING: ../src/learner.cc:576: \nParameters: { \"sublsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\ttrain-rmse:10.94265\teval-rmse:10.96235\n[20]\ttrain-rmse:3.93987\teval-rmse:3.95303\n[40]\ttrain-rmse:1.42968\teval-rmse:1.43881\n[60]\ttrain-rmse:0.53179\teval-rmse:0.54180\n[80]\ttrain-rmse:0.21566\teval-rmse:0.23392\n[100]\ttrain-rmse:0.11210\teval-rmse:0.14442\n[120]\ttrain-rmse:0.08046\teval-rmse:0.12337\n[140]\ttrain-rmse:0.06756\teval-rmse:0.11737\n[160]\ttrain-rmse:0.06052\teval-rmse:0.11537\n[180]\ttrain-rmse:0.05551\teval-rmse:0.11432\n[200]\ttrain-rmse:0.05131\teval-rmse:0.11363\n[220]\ttrain-rmse:0.04744\teval-rmse:0.11339\n[240]\ttrain-rmse:0.04410\teval-rmse:0.11280\n[260]\ttrain-rmse:0.04164\teval-rmse:0.11248\n[280]\ttrain-rmse:0.03918\teval-rmse:0.11237\n[300]\ttrain-rmse:0.03756\teval-rmse:0.11221\n[320]\ttrain-rmse:0.03562\teval-rmse:0.11197\n[340]\ttrain-rmse:0.03414\teval-rmse:0.11184\n[360]\ttrain-rmse:0.03241\teval-rmse:0.11178\n[380]\ttrain-rmse:0.03078\teval-rmse:0.11176\n[400]\ttrain-rmse:0.02942\teval-rmse:0.11159\n[420]\ttrain-rmse:0.02811\teval-rmse:0.11149\n[440]\ttrain-rmse:0.02684\teval-rmse:0.11137\n[460]\ttrain-rmse:0.02592\teval-rmse:0.11124\n[480]\ttrain-rmse:0.02482\teval-rmse:0.11117\n[499]\ttrain-rmse:0.02405\teval-rmse:0.11127\n0.11126376004005849\n[13:34:22] WARNING: ../src/learner.cc:576: \nParameters: { \"sublsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\ttrain-rmse:10.95034\teval-rmse:10.94577\n[20]\ttrain-rmse:3.94217\teval-rmse:3.94380\n[40]\ttrain-rmse:1.43014\teval-rmse:1.43522\n[60]\ttrain-rmse:0.53137\teval-rmse:0.54066\n[80]\ttrain-rmse:0.21484\teval-rmse:0.24105\n[100]\ttrain-rmse:0.11071\teval-rmse:0.15958\n[120]\ttrain-rmse:0.07800\teval-rmse:0.14214\n[140]\ttrain-rmse:0.06510\teval-rmse:0.13792\n[160]\ttrain-rmse:0.05785\teval-rmse:0.13605\n[180]\ttrain-rmse:0.05269\teval-rmse:0.13559\n[200]\ttrain-rmse:0.04800\teval-rmse:0.13496\n[220]\ttrain-rmse:0.04491\teval-rmse:0.13436\n[240]\ttrain-rmse:0.04126\teval-rmse:0.13421\n[260]\ttrain-rmse:0.03872\teval-rmse:0.13366\n[280]\ttrain-rmse:0.03639\teval-rmse:0.13339\n[300]\ttrain-rmse:0.03434\teval-rmse:0.13321\n[320]\ttrain-rmse:0.03243\teval-rmse:0.13297\n[340]\ttrain-rmse:0.03066\teval-rmse:0.13279\n[360]\ttrain-rmse:0.02937\teval-rmse:0.13273\n[380]\ttrain-rmse:0.02769\teval-rmse:0.13266\n[400]\ttrain-rmse:0.02632\teval-rmse:0.13250\n[420]\ttrain-rmse:0.02493\teval-rmse:0.13243\n[430]\ttrain-rmse:0.02436\teval-rmse:0.13240\n0.13240155575456936\n[13:34:25] WARNING: ../src/learner.cc:576: \nParameters: { \"sublsample\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\ttrain-rmse:10.95371\teval-rmse:10.93854\n[20]\ttrain-rmse:3.94387\teval-rmse:3.92883\n[40]\ttrain-rmse:1.43079\teval-rmse:1.41872\n[60]\ttrain-rmse:0.53136\teval-rmse:0.52648\n[80]\ttrain-rmse:0.21469\teval-rmse:0.22444\n[100]\ttrain-rmse:0.11180\teval-rmse:0.13921\n[120]\ttrain-rmse:0.08130\teval-rmse:0.12099\n[140]\ttrain-rmse:0.06946\teval-rmse:0.11607\n[160]\ttrain-rmse:0.06130\teval-rmse:0.11406\n[180]\ttrain-rmse:0.05526\teval-rmse:0.11333\n[200]\ttrain-rmse:0.05112\teval-rmse:0.11275\n[220]\ttrain-rmse:0.04755\teval-rmse:0.11244\n[240]\ttrain-rmse:0.04451\teval-rmse:0.11221\n[260]\ttrain-rmse:0.04208\teval-rmse:0.11197\n[280]\ttrain-rmse:0.03944\teval-rmse:0.11193\n[300]\ttrain-rmse:0.03710\teval-rmse:0.11188\n[320]\ttrain-rmse:0.03520\teval-rmse:0.11181\n[340]\ttrain-rmse:0.03297\teval-rmse:0.11178\n[343]\ttrain-rmse:0.03263\teval-rmse:0.11179\n0.1118077319754463\n","output_type":"stream"}]},{"cell_type":"code","source":"# p.260 XGBoostとLightGBMの結果を組み合わせる\n# リスト4.92 XGBoostの予測結果とLightGBMの予測結果の平均をとる\npreds_ans = preds_exp_xgb * 0.5 + preds_exp * 0.5\nsubmission[\"SalePrice\"] = preds_ans\n\n# リスト4.94 予測結果をCSVファイルとして書き出す\n# submission.to_csv(\"houseprices_submit04.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:41:29.199047Z","iopub.execute_input":"2022-01-10T13:41:29.199712Z","iopub.status.idle":"2022-01-10T13:41:29.210966Z","shell.execute_reply.started":"2022-01-10T13:41:29.199675Z","shell.execute_reply":"2022-01-10T13:41:29.210133Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# p.252 リスト4.86 Optunaでハイパーパラメータを調整\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234, shuffle=False, stratify=None)\n\ndef objective(trial):\n    xgb_params = {\n        \"learning_rate\": 0.05,\n        \"seed\":1234,\n        \"max_depth\":trial.suggest_int(\"max_depth\", 3, 16),\n        \"colsample_bytree\":trial.suggest_uniform(\"colsample_bytree\", 0.2, 0.9),\n        \"sublsample\":trial.suggest_uniform(\"sublsample\", 0.2, 0.9),\n    }\n    \n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_eval = xgb.DMatrix(X_valid, label=y_valid)\n    evals = [(xgb_train, \"train\"), (xgb_eval, \"eval\")]\n    \n    model_xgb = xgb.train(xgb_params, xgb_train, evals=evals, num_boost_round=1000, early_stopping_rounds=20, verbose_eval=10,)\n    y_pred = model_xgb.predict(xgb_eval)\n    score = np.sqrt(mean_squared_error(y_valid, y_pred))\n    return score\n\n#study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n#study.optimize(objective, n_trials=50)\n#study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:22:30.208665Z","iopub.execute_input":"2022-01-10T13:22:30.208965Z","iopub.status.idle":"2022-01-10T13:22:30.220022Z","shell.execute_reply.started":"2022-01-10T13:22:30.208927Z","shell.execute_reply":"2022-01-10T13:22:30.219358Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# p.262 4.11 統計手法を用いて家を分類する\n# リスト4.95 欠損値のある行を削除する\ntrain_df_le_dn = train_df_le.dropna()\ntrain_df_le_dn\n\n# p.263 データの正規化を行う\n# リスト4.96 データの正規化\n","metadata":{"execution":{"iopub.status.busy":"2022-01-10T13:46:07.833434Z","iopub.execute_input":"2022-01-10T13:46:07.833984Z","iopub.status.idle":"2022-01-10T13:46:07.879993Z","shell.execute_reply.started":"2022-01-10T13:46:07.833935Z","shell.execute_reply":"2022-01-10T13:46:07.879171Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"        Id  MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  \\\n0        1          60        3         65.0     8450      1        3   \n1        2          20        3         80.0     9600      1        3   \n2        3          60        3         68.0    11250      1        0   \n3        4          70        3         60.0     9550      1        0   \n4        5          60        3         84.0    14260      1        0   \n...    ...         ...      ...          ...      ...    ...      ...   \n1455  1456          60        3         62.0     7917      1        3   \n1456  1457          20        3         85.0    13175      1        3   \n1457  1458          70        3         66.0     9042      1        3   \n1458  1459          20        3         68.0     9717      1        3   \n1459  1460          20        3         75.0     9937      1        3   \n\n     LandContour Utilities LotConfig  ... YrSold SaleType SaleCondition  \\\n0              3         0         4  ...   2008        8             4   \n1              3         0         2  ...   2007        8             4   \n2              3         0         4  ...   2008        8             4   \n3              3         0         0  ...   2006        8             0   \n4              3         0         2  ...   2008        8             4   \n...          ...       ...       ...  ...    ...      ...           ...   \n1455           3         0         4  ...   2007        8             4   \n1456           3         0         4  ...   2010        8             4   \n1457           3         0         4  ...   2010        8             4   \n1458           3         0         4  ...   2010        8             4   \n1459           3         0         4  ...   2008        8             4   \n\n     SalePrice hasHighFacility Age  TotalSF  Total_Bathrooms  hasPorch  \\\n0     208500.0               0   5   2566.0              4.0         1   \n1     181500.0               0  31   2524.0              3.0         1   \n2     223500.0               0   7   2706.0              4.0         1   \n3     140000.0               0  91   2473.0              2.0         1   \n4     250000.0               0   8   3343.0              4.0         1   \n...        ...             ...  ..      ...              ...       ...   \n1455  175000.0               0   8   2600.0              3.0         1   \n1456  210000.0               0  32   3615.0              3.0         1   \n1457  266500.0               1  69   3492.0              2.0         1   \n1458  142125.0               0  60   2156.0              2.0         1   \n1459  147500.0               0  43   2512.0              3.0         1   \n\n      SalePrice_log  \n0         12.247694  \n1         12.109011  \n2         12.317167  \n3         11.849398  \n4         12.429216  \n...             ...  \n1455      12.072541  \n1456      12.254863  \n1457      12.493130  \n1458      11.864462  \n1459      11.901583  \n\n[1445 rows x 84 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>MSSubClass</th>\n      <th>MSZoning</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>Street</th>\n      <th>LotShape</th>\n      <th>LandContour</th>\n      <th>Utilities</th>\n      <th>LotConfig</th>\n      <th>...</th>\n      <th>YrSold</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n      <th>SalePrice</th>\n      <th>hasHighFacility</th>\n      <th>Age</th>\n      <th>TotalSF</th>\n      <th>Total_Bathrooms</th>\n      <th>hasPorch</th>\n      <th>SalePrice_log</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>60</td>\n      <td>3</td>\n      <td>65.0</td>\n      <td>8450</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>4</td>\n      <td>208500.0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>2566.0</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>12.247694</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>20</td>\n      <td>3</td>\n      <td>80.0</td>\n      <td>9600</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2007</td>\n      <td>8</td>\n      <td>4</td>\n      <td>181500.0</td>\n      <td>0</td>\n      <td>31</td>\n      <td>2524.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>12.109011</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>60</td>\n      <td>3</td>\n      <td>68.0</td>\n      <td>11250</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>4</td>\n      <td>223500.0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2706.0</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>12.317167</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>70</td>\n      <td>3</td>\n      <td>60.0</td>\n      <td>9550</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2006</td>\n      <td>8</td>\n      <td>0</td>\n      <td>140000.0</td>\n      <td>0</td>\n      <td>91</td>\n      <td>2473.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>11.849398</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>60</td>\n      <td>3</td>\n      <td>84.0</td>\n      <td>14260</td>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>4</td>\n      <td>250000.0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>3343.0</td>\n      <td>4.0</td>\n      <td>1</td>\n      <td>12.429216</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>1456</td>\n      <td>60</td>\n      <td>3</td>\n      <td>62.0</td>\n      <td>7917</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2007</td>\n      <td>8</td>\n      <td>4</td>\n      <td>175000.0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>2600.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>12.072541</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>1457</td>\n      <td>20</td>\n      <td>3</td>\n      <td>85.0</td>\n      <td>13175</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2010</td>\n      <td>8</td>\n      <td>4</td>\n      <td>210000.0</td>\n      <td>0</td>\n      <td>32</td>\n      <td>3615.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>12.254863</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1458</td>\n      <td>70</td>\n      <td>3</td>\n      <td>66.0</td>\n      <td>9042</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2010</td>\n      <td>8</td>\n      <td>4</td>\n      <td>266500.0</td>\n      <td>1</td>\n      <td>69</td>\n      <td>3492.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>12.493130</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>1459</td>\n      <td>20</td>\n      <td>3</td>\n      <td>68.0</td>\n      <td>9717</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2010</td>\n      <td>8</td>\n      <td>4</td>\n      <td>142125.0</td>\n      <td>0</td>\n      <td>60</td>\n      <td>2156.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>11.864462</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>1460</td>\n      <td>20</td>\n      <td>3</td>\n      <td>75.0</td>\n      <td>9937</td>\n      <td>1</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4</td>\n      <td>...</td>\n      <td>2008</td>\n      <td>8</td>\n      <td>4</td>\n      <td>147500.0</td>\n      <td>0</td>\n      <td>43</td>\n      <td>2512.0</td>\n      <td>3.0</td>\n      <td>1</td>\n      <td>11.901583</td>\n    </tr>\n  </tbody>\n</table>\n<p>1445 rows × 84 columns</p>\n</div>"},"metadata":{}}]}]}