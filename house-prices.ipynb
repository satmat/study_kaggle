{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# p.176 必要なライブラリをインポートする\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"ggplot\")\n\n# p.178 ランダムシードの設定\nimport random\nnp.random.seed(1234)\nrandom.seed(1234)\n\n# p.184 LabelEncoderのライブラリをインポート\nfrom sklearn.preprocessing import LabelEncoder\n\n# p.188 LightGBMのライブラリをインポート\nimport lightgbm as lgb\n\n# リスト4.17 LightGBMのハイパーパラメータを設定\nlgbm_params = {\n    \"objective\":\"regression\",\n    \"random_seed\":1234\n}\n\n# p.189 クロスバリデーション用のライブラリを読み込んで分割数を3に設定\nfrom sklearn.model_selection import KFold\nfolds = 3\nkf = KFold(n_splits=folds)\n\n# p.190 リスト4.19 平均二乗誤差を出すライブラリをインポート\nfrom sklearn.metrics import mean_squared_error\n\n# p.234 リスト4.65 Optunaのライブラリのインポート\nimport optuna\n\n# p.235 リスト4.66 学習データと検証データを作成\nfrom sklearn.model_selection import train_test_split\n\n# p.245 リスト4.76 ランダムフォレスト用のライブラリの読み込み\nfrom sklearn.ensemble import RandomForestRegressor as rf\n\n# p.251 リスト4.84 XGBoostのライブラリのインポート\nimport xgboost as xgb\n\n# p.279 決定木を可視化するためのライブラリをインストールする\n!pip install pydotplus\n\n# p.281 リスト4.115 ライブラリのインポート\nfrom sklearn import tree\nimport pydotplus\nfrom six import StringIO","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-12T15:30:36.038881Z","iopub.execute_input":"2022-01-12T15:30:36.039181Z","iopub.status.idle":"2022-01-12T15:30:49.363296Z","shell.execute_reply.started":"2022-01-12T15:30:36.039147Z","shell.execute_reply":"2022-01-12T15:30:49.362524Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting pydotplus\n  Downloading pydotplus-2.0.2.tar.gz (278 kB)\n     |████████████████████████████████| 278 kB 288 kB/s            \n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pyparsing>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from pydotplus) (3.0.6)\nBuilding wheels for collected packages: pydotplus\n  Building wheel for pydotplus (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pydotplus: filename=pydotplus-2.0.2-py3-none-any.whl size=24575 sha256=e17d5328b2f61186410b2dfbcb2638a0cd3462860a8e1af261030c9d3fb4fe3d\n  Stored in directory: /root/.cache/pip/wheels/1e/7b/04/7387cf6cc9e48b4a96e361b0be812f0708b394b821bf8c9c50\nSuccessfully built pydotplus\nInstalling collected packages: pydotplus\nSuccessfully installed pydotplus-2.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# p.179 CSVデータを読み込む\ntrain_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n\n# train_df.head()\n\n# p.180 学習データの各変数の型を確認する\n# train_df.dtypes\n\n# p.181 MSZoningの各分類ごとの個数を確認する\n# train_df[\"MSZoning\"].value_counts()\n\n# p.182 学習データとテストデータの連結\nall_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n# all_df\n\n# p.183 目的変数であるSalePriceの値を確認\n# all_df[\"SalePrice\"]\n\n# object型の変数を取得\ncategories = all_df.columns[all_df.dtypes == \"object\"]\n# print(categories)\n\n# p.185 'Alley'の各分類の個数を確認\n# all_df[\"Alley\"].value_counts()\n\n# 欠損値を数値に変換する\nfor cat in categories:\n    le = LabelEncoder()\n    # print(cat)\n    all_df[cat].fillna(\"missing\", inplace=True)\n    le = le.fit(all_df[cat])\n    all_df[cat] = le.transform(all_df[cat])\n    all_df[cat] = all_df[cat].astype(\"category\")\n# all_df\n\n# p.188 データをtrain_dfとtest_dfに戻す\n#train_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\n#test_df_le = all_df[all_df[\"SalePrice\"].isnull()]\n# train_df_le.head()\n# test_df_le.head()\n\n# リスト4.18 説明変数、目的変数を指定\n#train_X = train_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n#train_Y = train_df_le[\"SalePrice\"]\n\n# p.190 リスト4.20 各foldごとに作成したモデルごとの予測値を保存\n#models = []\n#rmses = []\n#oof = np.zeros(len(train_X))\n\n#for train_index, val_index in kf.split(train_X):\n#    X_train = train_X.iloc[train_index]\n#    X_valid = train_X.iloc[val_index]\n#    y_train = train_Y.iloc[train_index]\n#    y_valid = train_Y.iloc[val_index]\n    \n#    lgb_train = lgb.Dataset(X_train, y_train)\n#    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n#    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n#    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n#    tmp_rmse = np.sqrt(mean_squared_error(np.log(y_valid), np.log(y_pred)))\n#    print(tmp_rmse)\n    \n#    models.append(model_lgb)\n#    rmses.append(tmp_rmse)\n#    oof[val_index] = y_pred\n    \n# p.192 平均RMSEを計算する\n# sum(rmses)/len(rmses)\n\n# p.193 statisticsライブラリから計算\n#from statistics import mean\n#mean(rmses)\n\n# p.193 現状の予測値と実際の値の違いを可視化\n#actual_pred_df = pd.DataFrame({\"actual\": train_Y, \"pred\": oof})\n#actual_pred_df.plot(figsize=(12,5))\n\n# p.194 各変数の重要度を確認する\n# リスト4.24 変数の数を制限して各変数の重要度を表示\n#for model in models:\n#    lgb.plot_importance(model, importance_type=\"gain\", max_num_features=15)\n\n# p.197 リスト4.25 SalePriceの各統計量を確認する\n#train_df[\"SalePrice\"].describe()\n\n# p.198 リスト4.26 ヒストグラムで分布を確認\n#train_df[\"SalePrice\"].plot.hist(bins=20)\n\n# p.199 リスト4.27 SalePriceを対数化\n#np.log(train_df['SalePrice'])\n\n# 対数化したSalePriceの分布をヒストグラムで可視化\n#np.log(train_df['SalePrice']).plot.hist(bins=20)\n\n# p.200 リスト4.29 対数化による予測精度の向上を確認\n#train_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\n#train_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\n#train_Y = train_df_le[\"SalePrice_log\"]\n\n#models = []\n#rmses = []\n#oof = np.zeros(len(train_X))\n\n#for train_index, val_index in kf.split(train_X):\n#    X_train = train_X.iloc[train_index]\n#    X_valid = train_X.iloc[val_index]\n#    y_train = train_Y.iloc[train_index]\n#    y_valid = train_Y.iloc[val_index]\n    \n#    lgb_train = lgb.Dataset(X_train, y_train)\n#    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n#    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n#    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n#    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n#    print(tmp_rmse)\n    \n#    models.append(model_lgb)\n#    rmses.append(tmp_rmse)\n#    oof[val_index] = y_pred\n    \n#sum(rmses)/len(rmses)\n\n# p.204 リスト4.30 all_dfの作成\nall_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\ncategories = all_df.columns[all_df.dtypes == \"object\"]\n#print(categories)\n\n# p.205 リスト4.31 欠損値の数が上位40の変数を確認\n#all_df.isnull().sum().sort_values(ascending=False).head(40)\n\n# p.207 リスト4.32 PoolQCの各分類ごとの個数\n#all_df.PoolQC.value_counts()\n\n# リスト4.33 PoolQCの値を値があるものを1、値がないものを0に変換\n#all_df.loc[~all_df[\"PoolQC\"].isnull(), \"PoolQC\"] = 1\n#all_df.loc[all_df[\"PoolQC\"].isnull(), \"PoolQC\"] = 0\n# all_df.PoolQC.value_counts()\n\n# p.208 リスト4.35 MiscFeature, Alleyも0と1に変換する\n#all_df.loc[~all_df[\"MiscFeature\"].isnull(), \"MiscFeature\"] = 1\n#all_df.loc[all_df[\"MiscFeature\"].isnull(), \"MiscFeature\"] = 0\n#all_df.loc[~all_df[\"Alley\"].isnull(), \"Alley\"] = 1\n#all_df.loc[~all_df[\"Alley\"].isnull(), \"Alley\"] = 0\n\n# リスト4.36 繰り返し処理はfor文でまとめる\nHighFacility_col = [\"PoolQC\", \"MiscFeature\", \"Alley\"]\nfor col in HighFacility_col:\n    if all_df[col].dtype == \"object\":\n        if len(all_df[all_df[col].isnull()]) > 0:\n            all_df.loc[~all_df[col].isnull(), col] = 1\n            all_df.loc[all_df[col].isnull(), col] = 0\n\n# p.209 リスト4.37 0か1の値に変換した各変数を足し合わせて、高級住宅設備の数という特徴量を作成\nall_df[\"hasHighFacility\"] = all_df[\"PoolQC\"] + all_df[\"MiscFeature\"] + all_df[\"Alley\"]\nall_df[\"hasHighFacility\"] = all_df[\"hasHighFacility\"].astype(int)\n# all_df.hasHighFacility.value_counts()\n\n# p.209 リスト4.39 もとのデータからPoolQC, MiscFeature, Alleyを削除\nall_df = all_df.drop([\"PoolQC\", \"MiscFeature\", \"Alley\"], axis=1)\n\n# p.211 リスト4.40 各変数の統計量を確認する\n#all_df.describe().T # 転置\n\n# p.212 リスト4.41 数値データのみの抜き出し\ntrain_df_num = train_df.select_dtypes(include=[np.number])\n\n# リスト4.42 比例尺度ではない変数\nnonratio_features = [\"Id\", \"MSSubClass\", \"OverallQual\", \"OverallCond\", \"YearRemodAdd\", \"MoSold\", \"YrSold\"]\n\n# リスト4.43 数値データからリスト4.43の変数を除いた比例尺度データ\nnum_features = sorted(list(set(train_df_num) - set(nonratio_features)))\n#num_features\n\n# p.213 リスト4.44 比例尺度の列のみを抜き出す\ntrain_df_num_rs = train_df_num[num_features]\n\n# p.214 リスト4.45 3/4分位数が0となる変数を確認\n#for col in num_features:\n#    if train_df_num_rs.describe()[col][\"75%\"] == 0:\n#        print(col, len(train_df_num_rs[train_df_num_rs[col] == 0]))\n\n# p.215 リスト4.46 ある特定の値のみしかとらないものを確認\n#for col in num_features:\n#    if train_df_num_rs[col].nunique() < 15:\n#        print(col, train_df_num_rs[col].nunique())\n        \n# リスト4.47 外れ値があるか確認\nfor col in num_features:\n    tmp_df = train_df_num_rs[(train_df_num_rs[col] > train_df_num_rs[col].mean() + train_df_num_rs[col].std()*3) | (train_df_num_rs[col] < train_df_num_rs[col].mean() - train_df_num_rs[col].std()*3)]\n#    print(col, len(tmp_df))\n \n# p.217 リスト4.48 BsmtFinSF1とSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"BsmtFinSF1\", y=\"SalePrice\")\n\n# リスト4.49 BsmtFinSF1が広いもののSalePriceが高くないものを確認\n#all_df[all_df[\"BsmtFinSF1\"] > 5000]\n\n# p.218 TotalBsmtSFとSalePriceの分布を可視化\n# all_df.plot.scatter(x=\"TotalBsmtSF\", y=\"SalePrice\")\n# all_df[all_df[\"TotalBsmtSF\"] > 6000]\n\n# p.219 GrLivAreaとSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"GrLivArea\", y=\"SalePrice\")\n#all_df[all_df[\"GrLivArea\"] > 5000]\n\n# p.219 1stFlrSFとSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"1stFlrSF\", y=\"SalePrice\")\n#all_df[all_df[\"1stFlrSF\"] > 4000]\n\n# p.221 リスト4.53 外れ値以外を抽出（テストデータはすべて抽出）\nall_df = all_df[(all_df['BsmtFinSF1'] < 2000) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['TotalBsmtSF'] < 3000) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['GrLivArea'] < 4500) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['1stFlrSF'] < 2500) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['LotArea'] < 100000) | (all_df['SalePrice'].isnull())]\n\n# リスト4.54 categoriesの中から除外した3つの変数を削除\ncategories = categories.drop([\"PoolQC\", \"MiscFeature\", \"Alley\"])\n\n# リスト4.55 欠損値をmissingに置き換えてall_dfのカテゴリ変数をcategoryに設定\nfor cat in categories:\n    le = LabelEncoder()\n    #print(cat)\n    \n    all_df[cat].fillna(\"missing\", inplace=True)\n    le = le.fit(all_df[cat])\n    all_df[cat] = le.transform(all_df[cat])\n    all_df[cat] = all_df[cat].astype(\"category\")\n\n\n# p.228 リスト4.58 特徴量を追加\nall_df[\"Age\"] = all_df[\"YrSold\"] - all_df[\"YearBuilt\"]\n       \n# p.230 リスト4.61 広さの変数から追加するもの\nall_df[\"TotalSF\"] = all_df[\"TotalBsmtSF\"] + all_df[\"1stFlrSF\"] + all_df[\"2ndFlrSF\"]\nall_df[\"Total_Bathrooms\"] = all_df[\"FullBath\"] + all_df[\"HalfBath\"] + all_df[\"BsmtFullBath\"] + all_df[\"BsmtHalfBath\"]\n\n# リスト4.62 Porchの広さの合計も特徴量として追加\nall_df[\"Total_PorchSF\"] = all_df[\"WoodDeckSF\"] + all_df[\"OpenPorchSF\"] + all_df[\"EnclosedPorch\"] + all_df[\"3SsnPorch\"] + all_df[\"ScreenPorch\"]\n\n# リスト4.63 Porchの広さの合計をPorchがあるかないかの0,1の値に変換\nall_df[\"hasPorch\"] = all_df[\"Total_PorchSF\"].apply(lambda x: 1 if x > 0 else 0)\nall_df = all_df.drop(\"Total_PorchSF\", axis=1)   \n\n\n# p.223 リスト4.56 train_df_leとtest_df_leに分割\ntrain_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\ntest_df_le = all_df[all_df[\"SalePrice\"].isnull()]\n\ntrain_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\ntrain_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\ntrain_Y = train_df_le[\"SalePrice_log\"]\n\n# p.239 得られたハイパーパラメータを設定してクロスバリデーション\n# 適用前 valid_0's l2: 0.0137713\n# 適用後 valid_0's l2: 0.0129672\nlgbm_params = {\n    'objective': 'regression',\n    'random_seed':1234,\n    'learning_rate':0.05,\n    'n_estimators':1000,\n    'num_leaves': 33,\n    'max_bin': 125,\n    'bagging_fraction': 0.7197362581993618,\n    'bagging_freq': 4,\n     'feature_fraction': 0.4684501358427995,\n     'min_data_in_leaf': 14,\n     'min_sum_hessian_in_leaf': 2\n}\n\n\nmodels = []\nrmses = []\noof = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    #print(tmp_rmse)\n    \n    models.append(model_lgb)\n    rmses.append(tmp_rmse)\n    oof[val_index] = y_pred\n\n# sum(rmses)/len(rmses)\n    \n# p.227 リスト4.57 時間に関する変数の統計量を確認\n#all_df[[\"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\",\"YrSold\"]].describe()\n\n# p.229 リスト4.60 広さに関する変数の統計量を確認\n#all_df[[\"LotArea\", \"MasVnrArea\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\", \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"LotFrontage\"]].describe()\n\n# p.241 Kaggleに結果をsubmitする\n# リスト4.69 テストデータを用意\ntest_X = test_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n\n# リスト4.70 クロスバリデーションごとの各モデルで予測値を算出\npreds = []\n\nfor model in models:\n    pred = model.predict(test_X)\n    preds.append(pred)\n    \n# p.242 リスト4.71 predsの平均を計算してpreds_meanとして取得\npreds_array = np.array(preds)\npreds_mean = np.mean(preds_array, axis=0)\n\n# リスト4.72 もとのスケールに戻す\npreds_exp = np.exp(preds_mean)\nlen(preds_exp)\n\n# リスト4.73 予測値をSalePriceの値として置き換え\nsubmission[\"SalePrice\"] = preds_exp\n\n# p.243 リスト4.75 CSVファイルとして書き出す\n# submission.to_csv(\"houseprices_submit01.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T15:13:13.503929Z","iopub.execute_input":"2022-01-12T15:13:13.504216Z","iopub.status.idle":"2022-01-12T15:13:15.450274Z","shell.execute_reply.started":"2022-01-12T15:13:13.504183Z","shell.execute_reply":"2022-01-12T15:13:15.449551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# p.234 Optunaを実装する\n\n# p.235 リスト4.66 学習データと検証データを作成\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234, shuffle=False, stratify=None)\n\n# p.235 リスト4.67 Optunaでハイパーパラメータを最適化する\ndef objective(trial):\n    params = {\n        \"Objective\":\"regression\",\n        \"random_seed\":1234,\n        \"learning_rate\":0.05,\n        \"n_estimators\":1000,\n        \n        \"num_leaves\":trial.suggest_int(\"num_leaves\", 4, 64),\n        \"max_bin\":trial.suggest_int(\"max_bin\", 50, 200),\n        \"bagging_fraction\":trial.suggest_uniform(\"bagging_fraction\", 0.4, 0.9),\n        \"bagging_freq\":trial.suggest_int(\"bagging_freq\", 1, 10),\n        \"feature_fraction\":trial.suggest_uniform(\"feature_fraction\", 0.4, 0.9),\n        \"min_data_in_leaf\":trial.suggest_int(\"min_data_in_leaf\", 2, 16),\n        \"min_sum_hessian_in_leaf\":trial.suggest_int(\"min_sum_hessian_in_leaf\", 1, 10),\n    }\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n    model_lgb = lgb.train(params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n    score = np.sqrt(mean_squared_error(y_valid, y_pred))\n    \n    return score\n\n#study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n#study.optimize(objective, n_trials=50)\n#study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-01-12T15:13:15.451378Z","iopub.execute_input":"2022-01-12T15:13:15.45509Z","iopub.status.idle":"2022-01-12T15:13:15.472923Z","shell.execute_reply.started":"2022-01-12T15:13:15.455043Z","shell.execute_reply":"2022-01-12T15:13:15.471914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# p.245 欠損値を含む変数を確認する\nhasnan_cat = []\n\nfor col in all_df.columns:\n    tmp_null_count = all_df[col].isnull().sum()\n    if (tmp_null_count > 0) & (col != \"SalePrice\"):\n        #print(col, tmp_null_count)\n        hasnan_cat.append(col)\n        \n# p.246 欠損値を含む変数の統計量を確認する\n# リスト4.78 hasnan_catに含まれる変数を確認\n# all_df[hasnan_cat].describe()\n\n# p.247 欠損値を各変数の中央値で補完する\nfor col in all_df.columns:\n    tmp_null_count = all_df[col].isnull().sum()\n    if (tmp_null_count > 0) & (col != \"SalePrice\"):\n        #print(col, tmp_null_count)\n        all_df[col] = all_df[col].fillna(all_df[col].median())\n\n# p.248 リスト4.80 SalePriceの対数をとって学習\ntrain_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\ntest_df_le = all_df[all_df[\"SalePrice\"].isnull()]\ntrain_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\n\ntrain_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\ntrain_Y = train_df_le[\"SalePrice_log\"]\n\nfolds = 3\nkdf = KFold(n_splits=folds)\nmodels_rf = []\nrmses_rf = []\noof_rf = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    model_rf = rf(\n        n_estimators=50,\n        random_state=1234\n    )\n    \n    model_rf.fit(X_train, y_train)\n    \n    y_pred = model_rf.predict(X_valid)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    #print(tmp_rmse)\n    \n    models_rf.append(model_rf)\n    rmses_rf.append(tmp_rmse)\n    oof_rf[val_index] = y_pred\n    \n#sum(rmses_rf)/len(rmses_rf)\n\n# p.249 リスト4.81 テストデータで各クロスバリデーションのモデルで予測値を算出\ntest_X = test_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n\npreds_rf = []\nfor model in models_rf:\n    pred = model.predict(test_X)\n    preds_rf.append(pred)\n    \npreds_array_rf = np.array(preds_rf)\npreds_mean_rf = np.mean(preds_array_rf, axis=0)\npreds_exp_rf = np.exp(preds_mean_rf)\nsubmission[\"SalePrice\"] = preds_exp_rf\n\n#submission.to_csv(\"houseprices_submit02.csv\", index=False)\n\n\n# p.251 XGBoostを実装する\n# リスト4.85 category変数をint型に変換する\ncategories = train_X.columns[train_X.dtypes == \"category\"]\n\nfor col in categories:\n    train_X[col] = train_X[col].astype(\"int8\")\n    test_X[col] = test_X[col].astype(\"int8\")\n    \n# p.255 リスト4.87 ハイパーパラメータの設定\nxgb_params = {\n    \"learning_rate\":0.05,\n    \"seed\":1234,\n    \"max_depth\": 5,\n    \"colsample_bytree\": 0.3129166625194974,\n    \"sublsample\": 0.6571758278257789\n}\n\n# p.256 XGBoostでモデルを学習する\n# リスト4.88 最適化の処理\nmodels_xgb = []\nrmses_xgb = []\noof_xgb = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    \n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_eval = xgb.DMatrix(X_valid, label=y_valid)\n    evals = [(xgb_train, \"train\"), (xgb_eval, \"eval\")]\n    \n    model_xgb = xgb.train(xgb_params, xgb_train, evals=evals, num_boost_round=1000, early_stopping_rounds=20, verbose_eval=20,)\n    \n    y_pred = model_xgb.predict(xgb_eval)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    print(tmp_rmse)\n    \n    models_xgb.append(model_xgb)\n    rmses_xgb.append(tmp_rmse)\n    oof_xgb[val_index] = y_pred\n\nsum(rmses_xgb)/len(rmses_xgb)\n# 0.11849101592335805\n\n# 結果をCSVファイルとして書き出す\n# p.258 リスト4.89 テストデータでの予測値を算出\nxgb_test = xgb.DMatrix(test_X)\npreds_xgb = []\nfor model in models_xgb:\n    pred = model.predict(xgb_test)\n    preds_xgb.append(pred)\npreds_array_xgb = np.array(preds_xgb)\npreds_mean_xgb = np.mean(preds_array_xgb, axis=0)\npreds_exp_xgb = np.exp(preds_mean_xgb)\nsubmission[\"SalePrice\"] = preds_exp_xgb\n\n# p.259 リスト4.91\n# submission.to_csv(\"houseprices_submit03.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T15:13:15.474818Z","iopub.execute_input":"2022-01-12T15:13:15.475063Z","iopub.status.idle":"2022-01-12T15:13:27.899819Z","shell.execute_reply.started":"2022-01-12T15:13:15.475034Z","shell.execute_reply":"2022-01-12T15:13:27.899137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# p.260 XGBoostとLightGBMの結果を組み合わせる\n# リスト4.92 XGBoostの予測結果とLightGBMの予測結果の平均をとる\npreds_ans = preds_exp_xgb * 0.5 + preds_exp * 0.5\nsubmission[\"SalePrice\"] = preds_ans\n\n# リスト4.94 予測結果をCSVファイルとして書き出す\n# submission.to_csv(\"houseprices_submit04.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-12T15:13:27.900867Z","iopub.execute_input":"2022-01-12T15:13:27.904149Z","iopub.status.idle":"2022-01-12T15:13:27.910133Z","shell.execute_reply.started":"2022-01-12T15:13:27.904091Z","shell.execute_reply":"2022-01-12T15:13:27.909503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# p.252 リスト4.86 Optunaでハイパーパラメータを調整\nX_train, X_valid, y_train, y_valid = train_test_split(train_X, train_Y, test_size=0.2, random_state=1234, shuffle=False, stratify=None)\n\ndef objective(trial):\n    xgb_params = {\n        \"learning_rate\": 0.05,\n        \"seed\":1234,\n        \"max_depth\":trial.suggest_int(\"max_depth\", 3, 16),\n        \"colsample_bytree\":trial.suggest_uniform(\"colsample_bytree\", 0.2, 0.9),\n        \"sublsample\":trial.suggest_uniform(\"sublsample\", 0.2, 0.9),\n    }\n    \n    xgb_train = xgb.DMatrix(X_train, label=y_train)\n    xgb_eval = xgb.DMatrix(X_valid, label=y_valid)\n    evals = [(xgb_train, \"train\"), (xgb_eval, \"eval\")]\n    \n    model_xgb = xgb.train(xgb_params, xgb_train, evals=evals, num_boost_round=1000, early_stopping_rounds=20, verbose_eval=10,)\n    y_pred = model_xgb.predict(xgb_eval)\n    score = np.sqrt(mean_squared_error(y_valid, y_pred))\n    return score\n\n#study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n#study.optimize(objective, n_trials=50)\n#study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-01-12T15:13:27.914142Z","iopub.execute_input":"2022-01-12T15:13:27.915415Z","iopub.status.idle":"2022-01-12T15:13:27.937403Z","shell.execute_reply.started":"2022-01-12T15:13:27.915361Z","shell.execute_reply":"2022-01-12T15:13:27.936349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# p.262 4.11 統計手法を用いて家を分類する\n# リスト4.95 欠損値のある行を削除する\ntrain_df_le_dn = train_df_le.dropna()\n#train_df_le_dn\n\n# p.263 データの正規化を行う\n# リスト4.96 データの正規化\nfrom sklearn import preprocessing\ntrain_scaled = preprocessing.scale(train_df_le_dn.drop([\"Id\"], axis=1))\n#train_scaled\n\n# p.264 リスト4.97 np.array形式をDataFrame形式に戻す処理\ntrain_scaled_df = pd.DataFrame(train_scaled)\ntrain_scaled_df.columns = train_df_le_dn.drop([\"Id\"], axis=1).columns\n#train_scaled_df\n\n# p.266 k-means用のライブラリをインポートする\n# リスト4.98 k-means用のライブラリのインポート\nfrom sklearn.cluster import KMeans\n# リスト4.99 ランダムシードを設定\nnp.random.seed(1234)\n\n# リスト4.100 クラスタ数を指定して分類\nhouse_cluster = KMeans(n_clusters=4).fit_predict(train_scaled)\n\n# p.267 リスト4.101 家ごとのクラスタ情報を追加\ntrain_scaled_df[\"km_cluster\"] = house_cluster\n\n# リスト4.102 クラスタごとのデータ数を確認\ntrain_scaled_df[\"km_cluster\"].value_counts()\n\n# リスト4.103 クラスタごとの特徴を可視化\ncluster_mean = train_scaled_df[[\"km_cluster\", \"SalePrice\", \"TotalSF\", \"OverallQual\", \"Age\", \"Total_Bathrooms\", \"YearRemodAdd\", \"GarageArea\", \"MSZoning\", \"OverallCond\", \"KitchenQual\", \"FireplaceQu\"]].groupby(\"km_cluster\").mean().reset_index()\n\n# リスト4.104 転置処理を施して可視化\ncluster_mean = cluster_mean.T\n#cluster_mean\n\n#cluster_mean[1:].plot(figsize=(12, 10), kind=\"barh\", subplots=True, layout=(1, 4), sharey=True)\n\n# p.271 主成分分析用のライブラリをインポートする\n# リスト4.105 PCAパッケージのインポート\nfrom sklearn.decomposition import PCA\n\n# p.272 リスト4.106 主成分の数を指定\npca = PCA(n_components=2)\nhouse_pca = pca.fit(train_scaled).transform(train_scaled)\n# house_pca\n\n# p.272 リスト4.107 出力結果をDataFrame形式に変換してもとのDataFrameと結合\nhouse_pca_df = pd.DataFrame(house_pca)\nhouse_pca_df.columns = [\"pca1\", \"pca2\"]\ntrain_scaled_df = pd.concat([train_scaled_df, house_pca_df], axis=1)\n#train_scaled_df\n\n# p.273 主成分分析の結果を可視化\nmy_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n#for cl in train_scaled_df['km_cluster'].unique():\n#    plt.scatter(train_scaled_df.loc[train_scaled_df[\"km_cluster\"] == cl, 'pca1'], train_scaled_df.loc[train_scaled_df[\"km_cluster\"] == cl, \"pca2\"], label=cl, c=my_colors[cl], alpha=0.6)\n#plt.legend()\n#plt.show()\n\n# リスト4.109 見やすいように転置（行と列を変換）\npca_comp_df = pd.DataFrame(pca.components_, columns=train_scaled_df.drop([\"km_cluster\", \"pca1\", \"pca2\"], axis=1).columns).T\npca_comp_df.columns = [\"pca1\", \"pca2\"]\n#pca_comp_df\n\n# p.276 ハイクラスな家の条件を分析・可視化する\n# リスト4.110 SalePriceの分布を確認\n#train_df_le['SalePrice'].plot.hist(bins=20)\n#train_df_le['SalePrice'].describe()\n\n# p.277 リスト4.111 上位10%の価格を確認\ntrain_df['SalePrice'].quantile(0.9)\n\n# p.278 リスト4.112 high_class変数を追加\ntrain_df_le.loc[train_df[\"SalePrice\"] >= 278000, \"high_class\"] = 1\n# リスト4.113 条件を満たさないものを0とする\ntrain_df_le[\"high_class\"] = train_df_le[\"high_class\"].fillna(0)\ntrain_df_le.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-12T15:20:11.258929Z","iopub.execute_input":"2022-01-12T15:20:11.259642Z","iopub.status.idle":"2022-01-12T15:20:11.439827Z","shell.execute_reply.started":"2022-01-12T15:20:11.25959Z","shell.execute_reply":"2022-01-12T15:20:11.438871Z"},"trusted":true},"execution_count":null,"outputs":[]}]}