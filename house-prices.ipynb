{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# p.176 必要なライブラリをインポートする\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"ggplot\")\n\n# p.178 ランダムシードの設定\nimport random\nnp.random.seed(1234)\nrandom.seed(1234)\n\n# p.184 LabelEncoderのライブラリをインポート\nfrom sklearn.preprocessing import LabelEncoder\n\n# p.188 LightGBMのライブラリをインポート\nimport lightgbm as lgb\n\n# リスト4.17 LightGBMのハイパーパラメータを設定\nlgbm_params = {\n    \"objective\":\"regression\",\n    \"random_seed\":1234\n}\n\n# p.189 クロスバリデーション用のライブラリを読み込んで分割数を3に設定\nfrom sklearn.model_selection import KFold\nfolds = 3\nkf = KFold(n_splits=folds)\n\n# p.190 リスト4.19 平均二乗誤差を出すライブラリをインポート\nfrom sklearn.metrics import mean_squared_error","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-05T14:37:23.869358Z","iopub.execute_input":"2022-01-05T14:37:23.870035Z","iopub.status.idle":"2022-01-05T14:37:25.924055Z","shell.execute_reply.started":"2022-01-05T14:37:23.869929Z","shell.execute_reply":"2022-01-05T14:37:25.923116Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"code","source":"# p.179 CSVデータを読み込む\ntrain_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\ntest_df = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\nsubmission = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/sample_submission.csv\")\n\n# train_df.head()\n\n# p.180 学習データの各変数の型を確認する\n# train_df.dtypes\n\n# p.181 MSZoningの各分類ごとの個数を確認する\n# train_df[\"MSZoning\"].value_counts()\n\n# p.182 学習データとテストデータの連結\nall_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\n# all_df\n\n# p.183 目的変数であるSalePriceの値を確認\n# all_df[\"SalePrice\"]\n\n# object型の変数を取得\n#categories = all_df.columns[all_df.dtypes == \"object\"]\n# print(categories)\n\n# p.185 'Alley'の各分類の個数を確認\n# all_df[\"Alley\"].value_counts()\n\n# 欠損値を数値に変換する\nfor cat in categories:\n    le = LabelEncoder()\n    # print(cat)\n    all_df[cat].fillna(\"missing\", inplace=True)\n    le = le.fit(all_df[cat])\n    all_df[cat] = le.transform(all_df[cat])\n    all_df[cat] = all_df[cat].astype(\"category\")\n# all_df\n\n# p.188 データをtrain_dfとtest_dfに戻す\n#train_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\n#test_df_le = all_df[all_df[\"SalePrice\"].isnull()]\n# train_df_le.head()\n# test_df_le.head()\n\n# リスト4.18 説明変数、目的変数を指定\n#train_X = train_df_le.drop([\"SalePrice\", \"Id\"], axis=1)\n#train_Y = train_df_le[\"SalePrice\"]\n\n# p.190 リスト4.20 各foldごとに作成したモデルごとの予測値を保存\n#models = []\n#rmses = []\n#oof = np.zeros(len(train_X))\n\n#for train_index, val_index in kf.split(train_X):\n#    X_train = train_X.iloc[train_index]\n#    X_valid = train_X.iloc[val_index]\n#    y_train = train_Y.iloc[train_index]\n#    y_valid = train_Y.iloc[val_index]\n    \n#    lgb_train = lgb.Dataset(X_train, y_train)\n#    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n#    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n#    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n#    tmp_rmse = np.sqrt(mean_squared_error(np.log(y_valid), np.log(y_pred)))\n#    print(tmp_rmse)\n    \n#    models.append(model_lgb)\n#    rmses.append(tmp_rmse)\n#    oof[val_index] = y_pred\n    \n# p.192 平均RMSEを計算する\n# sum(rmses)/len(rmses)\n\n# p.193 statisticsライブラリから計算\n#from statistics import mean\n#mean(rmses)\n\n# p.193 現状の予測値と実際の値の違いを可視化\n#actual_pred_df = pd.DataFrame({\"actual\": train_Y, \"pred\": oof})\n#actual_pred_df.plot(figsize=(12,5))\n\n# p.194 各変数の重要度を確認する\n# リスト4.24 変数の数を制限して各変数の重要度を表示\n#for model in models:\n#    lgb.plot_importance(model, importance_type=\"gain\", max_num_features=15)\n\n# p.197 リスト4.25 SalePriceの各統計量を確認する\n#train_df[\"SalePrice\"].describe()\n\n# p.198 リスト4.26 ヒストグラムで分布を確認\n#train_df[\"SalePrice\"].plot.hist(bins=20)\n\n# p.199 リスト4.27 SalePriceを対数化\n#np.log(train_df['SalePrice'])\n\n# 対数化したSalePriceの分布をヒストグラムで可視化\n#np.log(train_df['SalePrice']).plot.hist(bins=20)\n\n# p.200 リスト4.29 対数化による予測精度の向上を確認\n#train_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\n#train_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\n#train_Y = train_df_le[\"SalePrice_log\"]\n\n#models = []\n#rmses = []\n#oof = np.zeros(len(train_X))\n\n#for train_index, val_index in kf.split(train_X):\n#    X_train = train_X.iloc[train_index]\n#    X_valid = train_X.iloc[val_index]\n#    y_train = train_Y.iloc[train_index]\n#    y_valid = train_Y.iloc[val_index]\n    \n#    lgb_train = lgb.Dataset(X_train, y_train)\n#    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n#    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n#    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n#    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n#    print(tmp_rmse)\n    \n#    models.append(model_lgb)\n#    rmses.append(tmp_rmse)\n#    oof[val_index] = y_pred\n    \n#sum(rmses)/len(rmses)\n\n# p.204 リスト4.30 all_dfの作成\nall_df = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\ncategories = all_df.columns[all_df.dtypes == \"object\"]\n#print(categories)\n\n# p.205 リスト4.31 欠損値の数が上位40の変数を確認\n#all_df.isnull().sum().sort_values(ascending=False).head(40)\n\n# p.207 リスト4.32 PoolQCの各分類ごとの個数\n#all_df.PoolQC.value_counts()\n\n# リスト4.33 PoolQCの値を値があるものを1、値がないものを0に変換\n#all_df.loc[~all_df[\"PoolQC\"].isnull(), \"PoolQC\"] = 1\n#all_df.loc[all_df[\"PoolQC\"].isnull(), \"PoolQC\"] = 0\n# all_df.PoolQC.value_counts()\n\n# p.208 リスト4.35 MiscFeature, Alleyも0と1に変換する\n#all_df.loc[~all_df[\"MiscFeature\"].isnull(), \"MiscFeature\"] = 1\n#all_df.loc[all_df[\"MiscFeature\"].isnull(), \"MiscFeature\"] = 0\n#all_df.loc[~all_df[\"Alley\"].isnull(), \"Alley\"] = 1\n#all_df.loc[~all_df[\"Alley\"].isnull(), \"Alley\"] = 0\n\n# リスト4.36 繰り返し処理はfor文でまとめる\nHighFacility_col = [\"PoolQC\", \"MiscFeature\", \"Alley\"]\nfor col in HighFacility_col:\n    if all_df[col].dtype == \"object\":\n        if len(all_df[all_df[col].isnull()]) > 0:\n            all_df.loc[~all_df[col].isnull(), col] = 1\n            all_df.loc[all_df[col].isnull(), col] = 0\n\n# p.209 リスト4.37 0か1の値に変換した各変数を足し合わせて、高級住宅設備の数という特徴量を作成\nall_df[\"hasHighFacility\"] = all_df[\"PoolQC\"] + all_df[\"MiscFeature\"] + all_df[\"Alley\"]\nall_df[\"hasHighFacility\"] = all_df[\"hasHighFacility\"].astype(int)\n# all_df.hasHighFacility.value_counts()\n\n# p.209 リスト4.39 もとのデータからPoolQC, MiscFeature, Alleyを削除\nall_df = all_df.drop([\"PoolQC\", \"MiscFeature\", \"Alley\"], axis=1)\n\n# p.211 リスト4.40 各変数の統計量を確認する\n#all_df.describe().T # 転置\n\n# p.212 リスト4.41 数値データのみの抜き出し\ntrain_df_num = train_df.select_dtypes(include=[np.number])\n\n# リスト4.42 比例尺度ではない変数\nnonratio_features = [\"Id\", \"MSSubClass\", \"OverallQual\", \"OverallCond\", \"YearRemodAdd\", \"MoSold\", \"YrSold\"]\n\n# リスト4.43 数値データからリスト4.43の変数を除いた比例尺度データ\nnum_features = sorted(list(set(train_df_num) - set(nonratio_features)))\n#num_features\n\n# p.213 リスト4.44 比例尺度の列のみを抜き出す\ntrain_df_num_rs = train_df_num[num_features]\n\n# p.214 リスト4.45 3/4分位数が0となる変数を確認\n#for col in num_features:\n#    if train_df_num_rs.describe()[col][\"75%\"] == 0:\n#        print(col, len(train_df_num_rs[train_df_num_rs[col] == 0]))\n\n# p.215 リスト4.46 ある特定の値のみしかとらないものを確認\n#for col in num_features:\n#    if train_df_num_rs[col].nunique() < 15:\n#        print(col, train_df_num_rs[col].nunique())\n        \n# リスト4.47 外れ値があるか確認\nfor col in num_features:\n    tmp_df = train_df_num_rs[(train_df_num_rs[col] > train_df_num_rs[col].mean() + train_df_num_rs[col].std()*3) | (train_df_num_rs[col] < train_df_num_rs[col].mean() - train_df_num_rs[col].std()*3)]\n#    print(col, len(tmp_df))\n \n# p.217 リスト4.48 BsmtFinSF1とSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"BsmtFinSF1\", y=\"SalePrice\")\n\n# リスト4.49 BsmtFinSF1が広いもののSalePriceが高くないものを確認\n#all_df[all_df[\"BsmtFinSF1\"] > 5000]\n\n# p.218 TotalBsmtSFとSalePriceの分布を可視化\n# all_df.plot.scatter(x=\"TotalBsmtSF\", y=\"SalePrice\")\n# all_df[all_df[\"TotalBsmtSF\"] > 6000]\n\n# p.219 GrLivAreaとSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"GrLivArea\", y=\"SalePrice\")\n#all_df[all_df[\"GrLivArea\"] > 5000]\n\n# p.219 1stFlrSFとSalePriceの分布を可視化\n#all_df.plot.scatter(x=\"1stFlrSF\", y=\"SalePrice\")\n#all_df[all_df[\"1stFlrSF\"] > 4000]\n\n# p.221 リスト4.53 外れ値以外を抽出（テストデータはすべて抽出）\nall_df = all_df[(all_df['BsmtFinSF1'] < 2000) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['TotalBsmtSF'] < 3000) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['GrLivArea'] < 4500) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['1stFlrSF'] < 2500) | (all_df['SalePrice'].isnull())]\nall_df = all_df[(all_df['LotArea'] < 100000) | (all_df['SalePrice'].isnull())]\n\n# リスト4.54 categoriesの中から除外した3つの変数を削除\ncategories = categories.drop([\"PoolQC\", \"MiscFeature\", \"Alley\"])\n\n# リスト4.55 欠損値をmissingに置き換えてall_dfのカテゴリ変数をcategoryに設定\nfor cat in categories:\n    le = LabelEncoder()\n    print(cat)\n    \n    all_df[cat].fillna(\"missing\", inplace=True)\n    le = le.fit(all_df[cat])\n    all_df[cat] = le.transform(all_df[cat])\n    all_df[cat] = all_df[cat].astype(\"category\")\n\n# p.223 リスト4.56 train_df_leとtest_df_leに分割\ntrain_df_le = all_df[~all_df[\"SalePrice\"].isnull()]\ntest_df_le = all_df[all_df[\"SalePrice\"].isnull()]\n\ntrain_df_le[\"SalePrice_log\"] = np.log(train_df_le[\"SalePrice\"])\ntrain_X = train_df_le.drop([\"SalePrice\", \"SalePrice_log\", \"Id\"], axis=1)\ntrain_Y = train_df_le[\"SalePrice_log\"]\n\nmodels = []\nrmses = []\noof = np.zeros(len(train_X))\n\nfor train_index, val_index in kf.split(train_X):\n    X_train = train_X.iloc[train_index]\n    X_valid = train_X.iloc[val_index]\n    y_train = train_Y.iloc[train_index]\n    y_valid = train_Y.iloc[val_index]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n    model_lgb = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=100, early_stopping_rounds=20, verbose_eval=10,)\n    \n    y_pred = model_lgb.predict(X_valid, num_iteration=model_lgb.best_iteration)\n    tmp_rmse = np.sqrt(mean_squared_error(y_valid, y_pred))\n    print(tmp_rmse)\n    \n    models.append(model_lgb)\n    rmses.append(tmp_rmse)\n    oof[val_index] = y_pred\n    \n    sum(rmses)/len(rmses)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T14:52:31.656122Z","iopub.execute_input":"2022-01-05T14:52:31.656402Z","iopub.status.idle":"2022-01-05T14:52:32.941797Z","shell.execute_reply.started":"2022-01-05T14:52:31.656373Z","shell.execute_reply":"2022-01-05T14:52:32.939983Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"MSZoning\nStreet\nLotShape\nLandContour\nUtilities\nLotConfig\nLandSlope\nNeighborhood\nCondition1\nCondition2\nBldgType\nHouseStyle\nRoofStyle\nRoofMatl\nExterior1st\nExterior2nd\nMasVnrType\nExterQual\nExterCond\nFoundation\nBsmtQual\nBsmtCond\nBsmtExposure\nBsmtFinType1\nBsmtFinType2\nHeating\nHeatingQC\nCentralAir\nElectrical\nKitchenQual\nFunctional\nFireplaceQu\nGarageType\nGarageFinish\nGarageQual\nGarageCond\nPavedDrive\nFence\nSaleType\nSaleCondition\n[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2949\n[LightGBM] [Info] Number of data points in the train set: 963, number of used features: 72\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:240: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Start training from score 12.010615\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's l2: 0.0423077\n[20]\tvalid_0's l2: 0.0220026\n[30]\tvalid_0's l2: 0.0175682\n[40]\tvalid_0's l2: 0.0163798\n[50]\tvalid_0's l2: 0.0162547\n[60]\tvalid_0's l2: 0.0162622\n[70]\tvalid_0's l2: 0.0162514\nEarly stopping, best iteration is:\n[58]\tvalid_0's l2: 0.0161791\n0.12719701309461987\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001805 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2943\n[LightGBM] [Info] Number of data points in the train set: 963, number of used features: 72\n[LightGBM] [Info] Start training from score 12.018820\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's l2: 0.0467163\n[20]\tvalid_0's l2: 0.026693\n[30]\tvalid_0's l2: 0.0218687\n[40]\tvalid_0's l2: 0.0201122\n[50]\tvalid_0's l2: 0.0196764\n[60]\tvalid_0's l2: 0.0196786\n[70]\tvalid_0's l2: 0.0196393\n[80]\tvalid_0's l2: 0.0195357\n[90]\tvalid_0's l2: 0.0194547\n[100]\tvalid_0's l2: 0.0193847\nDid not meet early stopping. Best iteration is:\n[100]\tvalid_0's l2: 0.0193847\n0.13922901661972728\n[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001808 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2987\n[LightGBM] [Info] Number of data points in the train set: 964, number of used features: 73\n[LightGBM] [Info] Start training from score 12.021869\nTraining until validation scores don't improve for 20 rounds\n[10]\tvalid_0's l2: 0.0370497\n[20]\tvalid_0's l2: 0.0195352\n[30]\tvalid_0's l2: 0.016412\n[40]\tvalid_0's l2: 0.0155942\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n  _log_warning('Overriding the parameters from Reference Dataset.')\n/opt/conda/lib/python3.7/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n  _log_warning(f'{cat_alias} in param dict is overridden.')\n","output_type":"stream"},{"name":"stdout","text":"[50]\tvalid_0's l2: 0.0151884\n[60]\tvalid_0's l2: 0.0151752\n[70]\tvalid_0's l2: 0.0153067\nEarly stopping, best iteration is:\n[58]\tvalid_0's l2: 0.0151274\n0.12299343081439215\n","output_type":"stream"}]}]}